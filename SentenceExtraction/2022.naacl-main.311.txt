
Yung-Sung ChuangRumen DangovskiHongyin LuoYang ZhangShiyu Chang
Marin Solja ˇci´cShang-Wen LiWen-tau YihYoon KimJames Glass
Massachusetts Institute of TechnologyMeta AI
MIT-IBM Watson AI LabUC Santa Barbara
yungsung@mit.edu
Abstract
We propose DiffCSE, an unsupervised con-
trastive learning framework for learning sen-
tence embeddings. DiffCSE learns sentence
embeddings that are sensitive to the difference
between the original sentence and an edited sen-
tence, where the edited sentence is obtained by
stochastically masking out the original sentence
and then sampling from a masked language
model. We show that DiffSCE is an instance
of equivariant contrastive learning (Dangovski
et al., 2021), which generalizes contrastive
learning and learns representations that are in-
sensitive to certain types of augmentations and
sensitive to other “harmful” types of augmen-
tations. Our experiments show that DiffCSE
achieves state-of-the-art results among unsuper-
vised sentence representation learning methods,
outperforming unsupervised SimCSEby 2.3
absolute points on semantic textual similarity
tasks.
1 Introduction
Learning “universal” sentence representations that
capture rich semantic information and are at the
same time performant across a wide range of down-
stream NLP tasks without task-specific finetuning
is an important open issue in the field (Conneau
et al., 2017; Cer et al., 2018; Kiros et al., 2015;
Logeswaran and Lee, 2018; Giorgi et al., 2020;
Yan et al., 2021; Gao et al., 2021). Recent work
has shown that finetuning pretrained language mod-
els with contrastive learning makes it possible to
learn good sentence embeddings without any la-
beled data (Giorgi et al., 2020; Yan et al., 2021;
Gao et al., 2021). Contrastive learning uses multi-
ple augmentations on a single datum to construct
positive pairs whose representations are trained tobe more similar to one another than negative pairs.
While different data augmentations (random crop-
ping, color jitter, rotations, etc.) have been found to
be crucial for pretraining vision models (Chen et al.,
2020), such augmentations have generally been un-
successful when applied to contrastive learning of
sentence embeddings. Indeed, Gao et al. (2021)
find that constructing positive pairs via a simple
dropout-based augmentation works much better
than more complex augmentations such as word
deletions or replacements based on synonyms or
masked language models. This is perhaps unsur-
prising in hindsight; while the training objective
in contrastive learning encourages representations
to be invariant to augmentation transformations,
direct augmentations on the input (e.g., deletion,
replacement) often change the meaning of the sen-
tence. That is, ideal sentence embeddings should
notbe invariant to such transformations.
We propose to learn sentence representations
that are aware of, but not necessarily invariant to,
such direct surface-level augmentations. This is an
instance of equivariant contrastive learning (Dan-
govski et al., 2021), which improves vision repre-
sentation learning by using a contrastive loss on
insensitive image transformations (e.g., grayscale)
and a prediction loss on sensitive image trans-
formations (e.g., rotations). We operationalize
equivariant contrastive learning on sentences by us-
ing dropout-based augmentation as the insensitive
transformation (as in SimCSE (Gao et al., 2021))
and MLM-based word replacement as the sensitive
transformation. This results in an additional cross-
entropy loss based on the difference between the
original and the transformed sentence.
We conduct experiments on 7 semantic textual
similarity tasks (STS) and 7 transfer tasks from Sen-
tEval (Conneau and Kiela, 2018) and find that this
difference-based learning greatly improves over
standard contrastive learning. Our DiffCSE ap-
proach can achieve around 2.3% absolute improve-4207
ment on STS datasets over SimCSE, the previous
state-of-the-art model. We also conduct a set of
ablation studies to justify our designed architecture.
Qualitative study and analysis are also included to
look into the embedding space of DiffCSE.
2 Background and Related Work
2.1 Learning Sentence Embeddings
Learning universal sentence embeddings has been
studied extensively in prior work, including unsu-
pervised approaches such as Skip-Thought (Kiros
et al., 2015), Quick-Thought (Logeswaran and Lee,
2018) and FastSent (Hill et al., 2016), or supervised
methods such as InferSent (Conneau et al., 2017),
Universal Sentence Encoder (Cer et al., 2018) and
Sentence-BERT (Reimers and Gurevych, 2019).
Recently, researchers have focused on (unsuper-
vised) contrastive learning approaches such as Sim-
CLR (Chen et al., 2020) to learn sentence embed-
dings. SimCLR (Chen et al., 2020) learns image
representations by creating semantically close aug-
mentations for the same images and then pulling
these representations to be closer than represen-
tations of random negative examples. The same
framework can be adapted to learning sentence em-
beddings by designing good augmentation meth-
ods for natural language. ConSERT (Yan et al.,
2021) uses a combination of four data augmen-
tation strategies: adversarial attack, token shuf-
fling, cut-off, and dropout. DeCLUTR (Giorgi
et al., 2020) uses overlapped spans as positive ex-amples and distant spans as negative examples for
learning contrastive span representations. Finally,
SimCSE (Gao et al., 2021) proposes an extremely
simple augmentation strategy by just switching
dropout masks. While simple, sentence embed-
dings learned in this manner have been shown to be
better than other more complicated augmentation
methods.
2.2 Equivariant Contrastive Learning
DiffCSE is inspired by a recent generalization of
contrastive learning in computer vision (CV) called
equivariant contrastive learning (Dangovski et al.,
2021). We now explain how this CV technique can
be adapted to natural language.
Understanding the role of input transformations
is crucial for successful contrastive learning. Past
empirical studies have revealed useful transforma-
tions for contrastive learning, such as random re-
sized cropping and color jitter for computer vision
(Chen et al., 2020) and dropout for NLP (Gao et al.,
2021). Contrastive learning encourages representa-
tions to be insensitive to these transformations, i.e.
the encoder is trained to be invariant to a set of man-
ually chosen transformations. The above studies
in CV and NLP have also revealed transformations
that are harmful for contrastive learning. For ex-
ample, Chen et al. (2020) showed that making the
representations insensitive to rotations decreases
the ImageNet linear probe accuracy, and Gao et al.
(2021) showed that using an MLM to replace 15%
of the words drastically reduces performance on4208STS-B. While previous works simply omit these
transformations from contrastive pre-training, here
we argue that we should still make use of these
transformations by learning representations that
aresensitive (but not necessarily invariant) to such
transformations.
The notion of (in)sensitivity can be captured by
the more general property of equivariance in math-
ematics. Let Tbe a transformation from a group
Gand let T(x)denote the transformation of a sen-
tence x. Equivariance is the property that there is
an induced group transformation Ton the output
features (Dangovski et al., 2021):
f(T(x)) =T(f(x)).
In the special case of contrastive learning, T’s
target is the identity transformation, and we say
thatfis trained to be “invariant to T.” However,
invariance is just a trivial case of equivariance,
and we can design training objectives where T
is not the identity for some transformations (such
as MLM), while it is the identity for others (such
as dropout). Dangovski et al. (2021) show that
generalizing contrastive learning to equivariance in
this way improves the semantic quality of features
in CV , and here we show that the complementary
nature of invariance and equivariance extends to
the NLP domain. The key observation is that the
encoder should be equivariant to MLM-based aug-
mentation instead of being invariant. We can oper-
ationalize this by using a conditional discriminator
that combines the sentence representation with an
edited sentence, and then predicts the difference
between the original and edited sentences. This
is essentially a conditional version of the ELEC-
TRA model (Clark et al., 2020), which makes the
encoder equivariant to MLM by using a binary dis-
criminator which detects whether a token is from
the original sentence or from a generator. We hy-
pothesize that conditioning the ELECTRA model
with the representation from our sentence encoder
is a useful objective for encouraging fto be “equiv-
ariant to MLM.”
To the best of our knowledge, we are the first to
observe and highlight the above parallel between
CV and NLP. In particular, we show that equivari-
ant contrastive learning extends beyond CV , and
that it works for transformations even without al-
gebraic structures, such as diff operations on sen-
tences. Further, insofar as the canonical set of
useful transformations is less established in NLP
than is in CV , DiffCSE can serve as a diagnostictool for NLP researchers to discover useful trans-
formations.
3 Difference-based Contrastive Learning
Our approach is straightforward and can be seen as
combining the standard contrastive learning objec-
tive from SimCSE (Figure 1, left) with a difference
prediction objective which conditions on the sen-
tence embedding (Figure 1, right).
Given an unlabeled input sentence x, SimCSE
creates a positive example xfor it by applying
different dropout masks. By using the BERT
encoder f, we can obtain the sentence embedding
h=f(x)forx(see section 4 for how his ob-
tained). The training objective for SimCSE is:
L =−loge()
/summationtexte(),
where Nis the batch size for the input batch
{x}as we are using in-batch negative exam-
ples, sim(·,·)is the cosine similarity function, and
τis a temperature hyperparameter.
On the right-hand side of Figure 1 is a con-
ditional version of the difference prediction ob-
jective used in ELECTRA (Clark et al., 2020),
which contains a generator and a discrimina-
tor. Given a sentence of length T,x=
[x, x, ..., x], we first apply a random mask
m= [m, m, ..., m], m∈[0,1]onxto
obtain x=m·x. We use another pretrained MLM
as the generator Gto perform masked language
modeling to recover randomly masked tokens in x
to obtain the edited sentence x=G(x). Then,
we use a discriminator Dto perform the Replaced
Token Detection (RTD) task. For each token in the
sentence, the model needs to predict whether it has
been replaced or not. The cross-entropy loss for a
single sentence xis:
L=/summationdisplay/parenleftbigg
−1/parenleftig
x=x/parenrightig
logD/parenleftbig
x,h, t/parenrightbig
−1/parenleftig
x̸=x/parenrightig
log/parenleftbig
1−D/parenleftbig
x,h, t/parenrightbig/parenrightbig/parenrightbigg
And the training objective for a batch is L=/summationtextL. Finally we optimize these two losses
together with a weighting coefficient λ:
L=L +λ· L
The difference between our model and ELECTRA
is that our discriminator Disconditional , so it can4209use the information of xcompressed in a fixed-
dimension vector h=f(x). The gradient of D
can be backward-propagated into fthrough h. By
doing so, fwill be encouraged to make hinfor-
mative enough to cover the full meaning of x, so
thatDcan distinguish the tiny difference between
xandx. This approach essentially makes the con-
ditional discriminator perform a “diff operation”,
hence the name DiffCSE.
When we train our DiffCSE model, we fix the
generator G, and only the sentence encoder fand
the discriminator Dare optimized. After training,
we discard Dand only use f(which remains fixed)
to extract sentence embeddings to evaluate on the
downstream tasks.
4 Experiments
4.1 Setup
In our experiment, we follow the setting of unsu-
pervised SimCSE (Gao et al., 2021) and build our
model based on their PyTorch implementation.
We also use the checkpoints of BERT (Devlin et al.,
2019) and RoBERTa (Liu et al., 2019) as the initial-
ization of our sentence encoder f. We add an MLP
layer with Batch Normalization (Ioffe and Szegedy,
2015) (BatchNorm) on top of the [CLS] represen-
tation as the sentence embedding. We will compare
the model with/without BatchNorm in section 5.
For the discriminator D, we use the same model
as the sentence encoder f(BERT/RoBERTa). For
the generator G, we use the smaller DistilBERT
and DistilRoBERTa (Sanh et al., 2019) for effi-
ciency. Note that the generator is fixed during train-
ing unlike the ELECTRA paper (Clark et al., 2020).
We will compare the results of using different size
model for the generator in section 5. More training
details are shown in Appendix A.
4.2 Data
For unsupervised pretraining, we use the same
10randomly sampled sentences from English
Wikipedia that are provided by the source code
of SimCSE.We evaluate our model on 7 seman-
tic textual similarity (STS) and 7 transfer tasks
in SentEval.STS tasks includes STS 2012–
2016 (Agirre et al., 2016), STS Benchmark (Cer
et al., 2017) and SICK-Relatedness (Marelli et al.,
2014). All the STS experiments are fully unsu-
pervised, which means no STS training datasetsare used and all embeddings are fixed once they
are trained. The transfer tasks are various sen-
tence classification tasks, including MR (Pang and
Lee, 2005), CR (Hu and Liu, 2004), SUBJ (Pang
and Lee, 2004), MPQA (Wiebe et al., 2005), SST-
2 (Socher et al., 2013), TREC (V oorhees and Tice,
2000) and MRPC (Dolan and Brockett, 2005). In
these transfer tasks, we will use a logistic regres-
sion classifier trained on top of the frozen sentence
embeddings, following the standard setup (Con-
neau and Kiela, 2018).
4.3 Results
Baselines We compare our model with many
strong unsupervised baselines including Sim-
CSE (Gao et al., 2021), IS-BERT (Zhang
et al., 2020), CMLM (Yang et al., 2020), De-
CLUTR (Giorgi et al., 2020), CT-BERT (Carlsson
et al., 2021), SG-OPT (Kim et al., 2021) and some
post-processing methods like BERT-flow (Li et al.,
2020) and BERT-whitening (Su et al., 2021) along
with some naive baselines like averaged GloVe em-
beddings (Pennington et al., 2014) and averaged
first and last layer BERT embeddings.
Semantic Textual Similarity (STS) We show
the results of STS tasks in Table 1 including
BERT (upper part) and RoBERTa (lower
part). We also reproduce the previous state-of-
the-art SimCSE (Gao et al., 2021). DiffCSE-
BERT can significantly outperform SimCSE-
BERT and raise the averaged Spearman’s cor-
relation from 76.25% to 78.49%. For the RoBERTa
model, DiffCSE-RoBERTa can also improve
upon SimCSE-RoBERTa from 76.57% to
77.80%.
Transfer Tasks We show the results of trans-
fer tasks in Table 2. Compared with SimCSE-
BERT, DiffCSE-BERT can improve the
averaged scores from 85.56% to 86.86%. When
applying it to the RoBERTa model, DiffCSE-
RoBERTa also improves upon SimCSE-
RoBERTa from 84.84% to 87.04%. Note that
the CMLM-BERT (Yang et al., 2020) can
achieve even better performance than DiffCSE.
However, they use 1TB of the training data from
Common Crawl dumps while our model only use
115MB of the Wikipedia data for pretraining. We
put their scores in Table 2 for reference. In Sim-
CSE, the authors propose to use MLM as an auxil-
iary task for the sentence encoder to further boost
the performance of transfer tasks. Compared with4210
the results of SimCSE with MLM, DiffCSE still
can have a little improvement around 0.2%.
5 Ablation Studies
In the following sections, we perform an extensive
series of ablation studies that support our model
design. We use BERT model to evaluate on
the development set of STS-B and transfer tasks.
Removing Contrastive Loss In our model, both
the contrastive loss and the RTD loss are crucial
because they maintain what should be sensitive and
what should be insensitive respectively. If we re-
move the RTD loss, the model becomes a SimCSE
model; if we remove the contrastive loss, the perfor-
mance of STS-B drops significantly by 30%, while
the average score of transfer tasks also drops by 2%
(see Table 3). This result shows that it is important
to have insensitive and sensitive attributes that exist
together in the representation space.
Next Sentence vs. Same Sentence Some meth-
ods for unsupervised sentence embeddings like
Quick-Thoughts (Logeswaran and Lee, 2018) and
CMLM (Yang et al., 2020) predict the next sen-
tence as the training objective. We also experi-
ment with a variant of DiffCSE by conditioning
the ELECTRA loss based on the next sentence.
Note that this kind of model is not doing a “diff
operation” between two similar sentences, and is
not an instance of equivariant contrastive learning.
As shown in Table 3 (use next sent. for x), the
score of STS-B decreases significantly compared
to DiffCSE while transfer performance remainssimilar. We also tried using the same sentence and
the next sentence at the same time for conditioning
the ELECTRA objective (use same+next sent. for
x), and did not observe improvements.
Other Conditional Pretraining Tasks Instead
of a conditional binary difference prediction loss,
we can also consider other conditional pretraining
tasks such as a conditional MLM objective pro-
posed by Yang et al. (2020), or corrective language
modeling,proposed by COCO-LM (Meng et al.,
2021). We experiment with these objectives instead
of the difference prediction objective in Table 3.
We observe that conditional MLM on the same sen-
tence does not improve the performance either on
STS-B or transfer tasks compared with DiffCSE.
Conditional MLM on the next sentence performs
even worse for STS-B, but slightly better than using
the same sentence on transfer tasks. Using both the
same and the next sentence also does not improve
the performance compared with DiffCSE. For the
corrective LM objective, the performance of STS-B
decreases significantly compared with DiffCSE.
Augmentation Methods: Insert/Delete/Replace
In DiffCSE, we use MLM token replacement as
the equivariant augmentation. It is possible to use
other methods like random insertion or deletion in-
stead of replacement.For insertion, we choose to4211
randomly insert mask tokens to the sentence, and
then use a generator to convert mask tokens into
real tokens. The number of inserted masked tokens
is 15% of the sentence length. The task is to predictwhether a token is an inserted token or the original
token. For deletion, we randomly delete 15% to-
kens in the sentence, and the task is to predict for
each token whether a token preceding it has been
deleted or not. The results are shown in Table 4.
We can see that using either insertion or deletion
achieves a slightly worse STS-B performance than
using MLM replacement. For transfer tasks, their
results are similar. Finally, we find that combining
all three augmentations in the training process does
not improve the MLM replacement strategy.
Pooler Choice In SimCSE, the authors use the
pooler in BERT’s original implementation (one
linear layer with tanh activation function) as the
final layer to extract features for computing con-
trastive loss. In our implementation (see details
in Appendix A), we find that it is better to use a
two-layer pooler with Batch Normalization (Batch-
Norm) (Ioffe and Szegedy, 2015), which is com-
monly used in contrastive learning framework in
computer vision (Chen et al., 2020; Grill et al.,
2020; Chen and He, 2021; Hua et al., 2021). We
show the ablation results in Table 5. We can ob-
serve that adding BatchNorm is beneficial for either
DiffCSE or SimCSE to get better performance on
STS-B and transfer tasks.
Size of the Generator In our DiffCSE model,
the generator can be in different model size
from BERT , BERT (Devlin et al., 2019),
DistilBERT (Sanh et al., 2019), BERT ,
BERT , BERT, BERT (Turc et al.,
2019). Their exact sizes are shown in Table 6 (L:
number of layers, H: hidden dimension). Notice
that although DistilBERT has only half the
number of layers of BERT, it can retain 97% of4212
BERT’s performance due to knowledge distillation.
We show our results in Table 6, we can see
the performance of transfer tasks does not change
much with different generators. However, the score
of STS-B decreases as we switch from BERT-
medium to BERT-tiny. This finding is not the same
as ELECTRA, which works best with generators
1/4-1/2 the size of the discriminator. Because our
discriminator is conditional on sentence vectors,
it will be easier for the discriminator to perform
the RTD task. As a result, using stronger gen-
erators (BERT, DistilBERT) to increase
the difficulty of RTD would help the discriminator
learn better. However, when using a large model
like BERT , it may be a too-challenging task
for the discriminator. In our experiment, using
DistilBERT, which has the ability close to but
slightly worse than BERT, gives us the best
performance.
Masking Ratio In our conditional ELECTRA
task, we can mask the original sentence in different
ratios for the generator to produce MLM-based
augmentations. A higher masking ratio will make
more perturbations to the sentence. Our empirical
result in Table 7 shows that the difference between
difference masking ratios is small (in 15%-40% ),
and a masking ratio of around 30% can give us the
best performance.
Coefficient λIn Section 3, we use the λcoeffi-
cient to weight the ELECTRA loss and then add it
with contrastive loss. Because the contrastive learn-
ing objective is a relatively easier task, the scale of
contrastive loss will be 100 to 1000 smaller than
ELECTRA loss. As a result, we need a smaller
λto balance these two loss terms. In the Table 8
we show the STS-B result under different λvalues.
Note that when λgoes to zero, the model becomes
a SimCSE model. We find that using λ= 0.005
can give us the best performance.
6 Analysis
6.1 Qualitative Study
A very common application for sentence embed-
dings is the retrieval task. Here we show some
retrieval examples to qualitatively explain why Dif-
fCSE can perform better than SimCSE. In this
study, we use the 2758 sentences from STS-B test-
ing set as the corpus, and then use sentence query
to retrieve the nearest neighbors in the sentence
embedding space by computing cosine similarities.
We show the retrieved top-3 examples in Table 9.
The first query sentence is “you can do it, too.”. The
SimCSE model retrieves a very similar sentence
but has a slightly different meaning (“you can use
it, too.”) as the rank-1 answer. In contrast, DiffCSE
can distinguish the tiny difference, so it retrieves
the ground truth answer as the rank-1 answer. The
second query sentence is “this is not a problem”.
SimCSE retrieves a sentence with opposite mean-
ing but very similar wording, while DiffCSE can
retrieve the correct answer with less similar word-
ing. We also provide a third example where both
SimCSE and DiffCSE fail to retrieve the correct
answer for a query sentence using double negation.
6.2 Retrieval Task
Besides the qualitative study, we also show the
quantitative result of the retrieval task. Here we
also use all the 2758 sentences in the testing set
of STS-B as the corpus. There are 97 positive
pairs in this corpus (with 5 out of 5 semantic sim-
ilarity scores from human annotation). For each
positive pair, we use one sentence to retrieve the
other one, and see whether the other sentence is
in the top-1/5/10 ranking. The recall@1/5/10 of
the retrieval task are shown in Table 10. We can
observe that DiffCSE can outperform SimCSE for4213
recall@1/5/10, showing the effectiveness of using
DiffCSE for the retrieval task.
6.3 Distribution of Sentence Embeddings
To look into the representation space of DiffCSE,
we plot the cosine similarity distribution of sen-
tence pairs from STS-B test set for both SimCSE
and DiffCSE in Figure 2. We observe that both
SimCSE and DiffCSE can assign cosine similari-
ties consistent with human ratings. However, we
also observe that under the same human rating,
DiffCSE assigns slightly higher cosine similari-
ties compared with SimCSE. This phenomenon
may be caused by the fact that ELECTRA and
other Transformer-based pretrained LMs have the
problem of squeezing the representation space, as
mentioned by Meng et al. (2021). As we use the
sentence embeddings as the input of ELECTRA to
perform conditional ELECTRA training, the sen-
tence embedding will be inevitably squeezed to
fit the input distribution of ELECTRA. We follow
prior studies (Wang and Isola, 2020; Gao et al.,
2021) to use uniformity andalignment (details in
Appendix C) to measure the quality of representa-
tion space for DiffCSE and SimCSE in Table 11.
Compared to averaged BERT embeddings, Sim-
CSE has similar alignment (0.177 v.s. 0.172) but
better uniformity (-2.313). In contrast, DiffCSE
has similar uniformity as Avg. BERT (-1.438 v.s.
-1.468) but much better alignment (0.097). It in-
dicates that SimCSE and DiffCSE are optimizing
the representation space in two different directions.
And the improvement of DiffCSE may come from
its better alignment.
7 Conclusion
In this paper, we present DiffCSE, a new unsu-
pervised sentence embedding framework that is
aware of, but not invariant to, MLM-based word
replacement. Empirical results on semantic textual
similarity tasks and transfer tasks both show the
effectiveness of DiffCSE compared to current state-
of-the-art sentence embedding methods. We also
conduct extensive ablation studies to demonstrate
the different modeling choices in DiffCSE. Quali-
tative study and the retrieval results also show that
DiffCSE can produce a better embedding space for
sentence retrieval. One limitation of our work is
that we do not explore the supervised setting that
uses human-labeled NLI datasets to further boost
the performance. We leave this topic for future
work. We believe that our work can provide re-
searchers in the NLP community a new way to
utilize augmentations for natural language and thus
produce better sentence embeddings.4214Acknowledgements
In this research, Yung-Sung and Hongyin were
partially supported by the Centre for Perceptual
and Interactive Intelligence (CPII) Ltd under the
Innovation and Technology Fund (InnoHK).
References42154216A Training Details
We use a single NVIDIA 2080Ti GPU for
each experiment. The averaged running time
for DiffCSE is 3-6 hours. We use grid-
search of batch size ∈ { 64,128}learning
rate∈ {2e-6, 3e-6, 5e-6, 7e-6, 1e-5 }and mask-
ing ratio ∈ {0.15,0.20,0.30,0.40}andλ∈
{0.1,0.05,0.01,0.005,0.001}. The temperature
τin SimCSE is set to 0.05 for all the experiments.
During the training process, we save the checkpoint
with the highest score on the STS-B development
set. And then we use STS-B development set to
find the best hyperparameters (listed in Table 12)
for STS task; we use the averaged score of the de-
velopment sets of 7 transfer tasks to find the best
hyperparameters (listed in Table 13) for transfer
tasks. All numbers in Table 1 and Table 2 are from
a single run.
During testing, we follow SimCSE to discard the
MLP projector and only use the [CLS] output to
extract the sentence embeddings.
The numbers of model parameters for BERT
and RoBERTa are listed in Table 14. Note that
in training time DiffCSE needs two BERT models
to work together (sentence encoder + discrimina-
tor), but in testing time we only need the sentence
encoder, so the model size is the same as the Sim-
CSE model.
Projector with BatchNorm In Section 5, we
mention that we use a projector with BatchNorm
as the final layer of our model. Here we provided
the PyTorch code for its structure:
B Using Augmentations as
Positive/Negative Examples
In Section 5, we try to use different augmentations
(e.g. insertion, deletion, replacement) for learning
equivariance. In Table 15 we provide the results of
using these augmentations as additional positive or
negative examples along with the SimCSE training
paradigm. We can observe that using these aug-
mentations as additional positives only decreases
the performance. The only method that can im-
prove the performance a little bit is to use MLM
15% replaced examples as additional negative ex-
amples. Overall, none of these results can perform4217better than our proposed method, e.g. using these
augmentations to learn equivariance.
C Uniformity and Alignment
Wang and Isola (2020) propose to use two prop-
erties, alignment anduniformity , to measure the
quality of representations. Given a distribution of
positive pairs pand the distribution of the whole
dataset p,alignment computes the expected
distance between normalized embeddings of the
paired sentences:
ℓ≜ E/vextenddouble/vextenddoublef(x)−f/parenleftbig
x/parenrightbig/vextenddouble/vextenddouble.
Uniformity measures how well the embeddings are
uniformly distributed in the representation space:
ℓ≜log Ee.
The smaller the values of uniformity and alignment,
the better the quality of the representation space is
indicated.
D Source Code
We build our model using the PyTorch implementa-
tion of SimCSEGao et al. (2021), which is based
on the HuggingFace’s Transformers package.We
also upload our codeand pretrained models (links
inREADME.md ). Please follow the instructions in
README.md to reproduce the results.
E Potential Risks
On the risk side, insofar as our method utilizes pre-
trained language models, it may inherit and prop-
agate some of the biases present in such models.
Besides that, we do not see any other potential risks
in our paper.4218