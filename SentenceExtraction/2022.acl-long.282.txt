
Yong Cheng, Ankur Bapna, Orhan Firat, Yuan Cao,
Pidong Wang , and Wolfgang Macherey
Google Research
{chengyong,ankurbpn,orhanf,yuancao,pidong,wmach}@google.com
Abstract
Multilingual neural machine translation mod-
els are trained to maximize the likelihood
of a mix of examples drawn from multiple
language pairs. The dominant inductive bias
applied to these models is a shared vocab-
ulary and a shared set of parameters across
languages; the inputs and labels correspond-
ing to examples drawn from different lan-
guage pairs might still reside in distinct sub-
spaces. In this paper, we introduce multilin-
gual crossover encoder-decoder ( mXEncDec )
to fuse language pairs at an instance level. Our
approach interpolates instances from different
language pairs into joint ‘crossover examples’
in order to encourage sharing input and output
spaces across languages. To ensure better fu-
sion of examples in multilingual settings, we
propose several techniques to improve exam-
ple interpolation across dissimilar languages
under heavy data imbalance. Experiments on a
large-scale WMT multilingual dataset demon-
strate that our approach signiﬁcantly improves
quality on English-to-Many, Many-to-English
and zero-shot translation tasks (from +0:5
BLEU up to +5:5BLEU points). Results on
code-switching sets demonstrate the capability
of our approach to improve model generaliza-
tion to out-of-distribution multilingual exam-
ples. We also conduct qualitative and quantita-
tive representation comparisons to analyze the
advantages of our approach at the representa-
tion level.
1 Introduction
Multilingual modeling has been receiving increas-
ing research attention over the past few years, aris-
ing from successful demonstrations of improved
quality across a variety of tasks, languages and
modalities (Lample and Conneau, 2019; Arivazha-
gan et al., 2019b; Conneau et al., 2021). The suc-
cess of these models is typically ascribed to vocabu-
lary sharing, parameter tying and implicit pivoting
through dominant languages like English (Conneauet al., 2020). These conventional techniques are
effective, but might not be exploiting the full poten-
tial of multilingual models to learn the underlying
inductive bias: the learning signal from one lan-
guage should beneﬁt the quality of other languages
(Caruana, 1997; Arivazhagan et al., 2019b).
Here we study two related issues that exist in the
context of multilingual Neural Machine Transla-
tion (NMT) training (Dong et al., 2015; Firat et al.,
2016a; Johnson et al., 2017). First, NMT mod-
els (Bahdanau et al., 2015; Vaswani et al., 2017) are
trained with maximum likelihood estimation which
has a strong tendency to overﬁt and even memo-
rize observed training examples, particularly pos-
ing challenges for low resource languages (Zhang
et al., 2018). Second, training examples from dis-
tinct language pairs are separately fed into multi-
lingual NMT models without any explicit instance-
level sharing (with the exception of multi-source
NMT (Zoph and Knight, 2016; Firat et al., 2016b));
as a consequence, given large enough capacity, the
models have the liberty to map representations of
different languages into distinct subspaces, limiting
the extent of cross-lingual transfer.
In this work, we introduce multilingual crossover
encoder-decoder ( mXEncDec ) to address these is-
sues following the recent work on XEncDec (Cheng
et al., 2021) and mixup (Zhang et al., 2018; Cheng
et al., 2020; Guo et al., 2020). Inspired by chromo-
somal crossovers (Rieger et al., 2012), mXEncDec
fuses two multilingual training examples to gen-
erate crossover examples inheriting the combina-
tions of traits of different language pairs, which
is capable of explicitly capturing cross-lingual sig-
nals compared to the standard training which me-
chanically combines multiple language pairs. mX-
EncDec has the following advantages:
1.Enhancing the cross-lingual generalization.
Thanks to crossover examples generated by
fusing different language pairs, the multilin-
gual NMT is encouraged to learn to transfer4092explicitly via more languages rather than im-
plicitly via the predominant languages.
2.Improving the model generalization and ro-
bustness. As vicinity examples around each
example in the multilingual corpus (akin to
Vicinal Risk Minimization (Chapelle et al.,
2001)), crossover examples produced by mX-
EncDec can enrich the support of the training
distribution and lead to better generalization
and robustness respectively on general and
noisy inputs (Zhang et al., 2018).
3.Alleviating overﬁtting to low-resource lan-
guages. mXEncDec can increase the diver-
sity of low-resource languages by fusing low-
resource examples with others, instead of the
simple duplication in the standard training.
InmXEncDec , we randomly pick up two train-
ing examples drawn from the multilingual training
corpus and ﬁrst interpolate their source sentences
where we have to prudently deal with language
tags. Then we leverage a mixture decoder to pro-
duce a virtual target sentence. To account for heavy
data imbalance of each language pair, we propose
a pairwise sampling strategy to adjust interpolation
ratios between language pairs. We also propose to
simplify the target interpolation to cope with noisy
attention and fusions of dissimilar language pairs.
Different from XEncDec fusing two heterogeneous
tasks (Cheng et al., 2021), we attempt to adapt it to
deeply fuse different language pairs.
Experimental results on a large-scale WMT mul-
tilingual dataset show that mXEncDec yields im-
provements of +1:13and+0:47BLEU points av-
eragely on xx-en and en-xx test sets over a vanilla
multilingual Transformer model. We also evalu-
ate our approaches on zero-shot translations and
obtain up to +5:53BLEU points over the base-
line method, which corroborates the better transfer-
abilty of multilingual models with our approaches.
The more stable performance on noisy input text
demonstrates the capability of our approach to im-
prove the model robustness. To further explain the
model behaviors at the representation level, quali-
tative and quantitative comparisons on representa-
tions manifest that our approach learns better multi-
lingual representations, which indirectly explicates
the BLEU improvements.
2 Background
Multilingual Neural Machine Translation .
NMT (Bahdanau et al., 2015; Vaswani et al., 2017)optimizes the conditional probability P(yjx;)
of translating a source-language sentence x
into a target-language sentence y. The encoder
reads the source sentence x=x;:::;xas
a sequence of word embeddings e(x). The
decoder acts as a conditional language model over
embeddings e(y)and the encoder outputs with a
cross-attention mechanism (Bahdanau et al., 2015).
For clarity, we denote the input and output in the
decoder as zandy,i.e.,z=hsi;y;;y
as a shifted copy of y, wherehsiis a sentence
start token. Then the decoder generates yas
P(yjx;) =QP(yjz;x;):The cross-
attention matrix is denoted as A2R. NMT
optimizes the parameters by maximizing the
likelihood of a parallel training set D:
L() = E[`(f(x;y;);v(y))]; (1)
where`is the cross entropy loss between the model
predictionf(x;y;)and label vectors v(y)fory.
v(y)could be a sequence of one-hot vectors with
smoothing in Transformer (Vaswani et al., 2017).
Multilingual NMT extends NMT from the bilin-
gual to the multilingual setting, in which it learns a
one-to-many, many-to-one or many-to-many map-
ping from a set of languages to another set of lan-
guages (Firat et al., 2016a; Johnson et al., 2017).
More speciﬁcally, the multilingual NMT model
is learned over parallel corpora M=fDg
whereLis the number of language pairs:
L() = EE[`(f(x;y;);v(y))];
(2)
where all the parallel training sets are fed into the
NMT model.
XEncdec: Crossover Encoder-Decoder .XEnc-
Decaims to fuse two parallel examples (called par-
ents) in the encoder-decoder model (Cheng et al.,
2021). The parents’ source sentences are shufﬂed
into a sentence (the offspring’s source) on the en-
coder side, and a mixture decoder model predicts
a virtual target sentence (the offspring’s target).
Given a pair of examples (x;y)and(x;y)where
their lengths are different in most cases, padding
tokens are appended to the shorter one to align their
lengths. The crossover example (~x;~y)(offspring)
is generated by carrying out XEncDec over(x;y)
and(x;y)(parents).
The crossover encoder combines embeddings of
the two source sequences into a new sequence of4093embeddings:
e(~x) =e(x)m+e(x)(1 m); (3)
where m=m;;m2f0;1gis sampled
from a distribution or constructed according to a
hyperparameter ratio p; e.g.,p= 0:15means that
15% of elements in mare0.j~xjis the length of ~x,
which is equal to max(jxj;jxj).
On the crossover decoder side, a mixture con-
ditional language model is employed for the gen-
eration of the virtual target sentence. The input
embedding e(~z)and output label v(~y)for the
decoder at the j-th position are calculated as:
e(~z) =e(y)t+e(y)(1 t);(4)
v(~y) =v(y)t+v(y)(1 t); (5)
where t=t;:::;t2[0;1]R. In con-
trast to a common language model fed with a single
wordyfor predicting yat thej-th position, the
crossover decoder aims to generate an interpolated
vectorv(~y)by averaging v(y)andv(y)witht,
on condition that the current input embedding is
also weighted on embeddings e(y)ande(y)
witht. The weight vector tused for interpolat-
ing target inputs and labels is computed as:
t=PAmPAm+PA(1 m);(6)
where AandAare the alignment matrices for
(x;y)and(x;y). In practice the cross-attention
scores in the NMT model are utilized as an alterna-
tive noisy alignment matrix (Garg et al., 2019).
The cross-entropy is utilized to compute the loss
forXEncDec when feeding e(~x),e(~z)andv(~y)
into the encoder-decoder model, denoted as:
`(f(~x;~y;);v(~y))
=XKL(v(~y)kP(yj~z;~x;)): (7)
3 mXEncDec
In this work, we aim to leverage XEncDec to en-
courage multilingual NMT models to better ex-
ploit cross-lingual signals with crossover exam-
ples created by explicitly fusing different language
pairs. We introduce its variant, called mXEncDec
as shown in Figure 1, in which the parent examples
could belong to either the same or different lan-
guage pairs. The subsequent subsections discuss
how to address new challenges of mXEncDec for
multilingual NMT.
Language Interpolation . As multilingual NMT
involves a large number of language pairs, several
techniques have been adopted to distinguish trans-
lation directions among them, such as prepending a
language tag to source inputs (Johnson et al., 2017)
or both source and target sentences (Wang et al.,
2018), training language-speciﬁc embeddings for
different languages (Lample and Conneau, 2019),
and so on (Dabre et al., 2020). When follow-
ing Lample and Conneau (2019), it is natural to
interpolate language-speciﬁc embeddings as we do
for token embeddings. However, if we want to
adopt a language tag in the ﬁrst word of a source
sentence to indicate the target language (Johnson
et al., 2017), we need to address how to interpolate
them. As Figure 1 shows, to make the sentence
~xstill carry language-speciﬁc information from x
andx, we conduct a soft combination over their
language tags, that is:
e(~x) =e(x)Pm
jmj 1+e(x)P(1 m)
jmj 1;
(8)
wherejmjis the length of m.e(~x)captures the
proportion of words in ~xcoming from the transla-
tion pairs ( x,y) and ( x,y).
Simpliﬁed Target Interpolation . In comparison
to bilingual NMT, attention matrices learned in
multilingual NMT models are excessively noisy,
which results in an inappropriate design of using
the attention-based target interpolation in Eq. (6)
formXEncDec . Instead, we can employ a simple
linear interpolation by setting tas a constant vector,
here exempliﬁed by the case of using language tags:
t=Pm
jmj 1;8j2f1;:::;j~yjg; (9)4094A similar equation can be obtained for using lan-
guage embeddings. In addition, dispensing with
attention can improve the parallel efﬁciency with
10% speed-up gain.
Hard Target Input Interpolation . For multilin-
gual NMT with multiple languages on the target
side, i.e., one-to-many and many-to-many models,
we need to carefully design combinations of target
input word embeddings. As representations from
the same language are usually close to each other,
it can still augment the representation space by lin-
early interpolating target embeddings in Eq. (4).
But for dissimilar languages, in particular distantly
related languages, the interpolation points between
them are comparatively unreliable. To tackle this
issue, we simply quantize tto1ift>0:5, other-
wiset= 0when interpolating target input embed-
dings for two different target languages in Eq. (4).
A better solution should consider varying the inter-
polation ratio based on the language similarity or
encourage interpolations of similar languages. We
leave this for future exploration.
Pairwise Sampling . The multilingual corpus is
usually heavily imbalanced: most of its data dis-
tribution concentrates on high-resource language
pairs (Arivazhagan et al., 2019b). When inter-
polating high-resource and low-resource sentence
pairs, we assume the fusion should be encouraged
to be in favor of high-resource language pairs be-
cause the representation space supported by high-
resource sentences is relatively reliable and stable
(Kudugunta et al., 2019). This indicates a more
frequent small p(e.g.p < 0:5) to weigh high-
resource sentences over low-resource sentences
if(x;y)2Dis a high-resource sentence and
(x;y)2Dis a low-resource sentence. To this
end, we propose a pairwise sampling method to
sample the source shufﬂe ratio pfor interpolat-
ing language pair landl:
gBernoulli (1=(1 +exp( d(l;l)));(10)
p=gp+ (1 g)(1 p); (11)
whereis a temperature hyperparameter to control
the tendency of gtowards 0or1for the Bernoulli
distribution. d(l;l)can be an arbitrary metric to
measure the relationship between language land
l. Here we use d(l;l) =jDj=jDjwherejDj
denotes the data size of the language pair l.
Computing Loss . We calculate the training lossAlgorithm 1: Computing mXEncDec Loss.
Input: CorpusM, temperature , ratiop.
Output: Batch LossL().Function mXEncDec (M,,p): (X;Y) shufﬂe (X;Y )2M along
batch; foreach (x;y;x;y)2(X;Y;X;Y)
dop sample a shufﬂe ratio in
Eq. (10) and (11) with andp; (e(~x);e(~z);v(~y)) compute them
using Eq. (3)-(5), (8), (6) or (9),
andp;L Eq. (7) with
(e(~x);e(~z);v(~y)); end returnL()
over mXEncDec as:
L() =EEEE
[`(f(~x;~y;);v(~y))]; (12)
where the generation of (~x;~y)depends on (x;y)
and(x;y). Algorithm 1 shows how to compute
Eq.(12) effectively. We shufﬂe the min-batch con-
sisting of all the language pairs. Then the shufﬂed
batch and original batch can be used to generate
(~x;~y)to compute the mXEncDec loss. Instead of
using one-hot labels v(y)in Eq. (5), we adopt la-
bel co-reﬁnement (Li et al., 2019) by linearly comb-
ing the ground-truth one-hot label with the model
prediction, that is v(y)+f(x;y;^)(1 ). Fi-
nally, our approach optimizes the model loss in-
volving two training losses, Eq. (2) and Eq. (12):
= argminfL() +L()g: (13)
4 Experiments
Data and Evaluation . We conduct experiments
on the English-centric WMT multilingual dataset
composed of 16languages (including English) and
30translation directions from past WMT evalua-
tion campaigns before and on WMT’ 19(Barrault
et al., 2019). The data distribution is highly skewed,
varying from roughly 10k examples in En-Gu to
roughly 60M examples in En-Cs. Two non-English
test sets, Fr-De and De-Cs, are used to verify zero-
shot translations. In addition, we also use multi-4095= -2 -0.8 -0.4 0 0.4 0.8 2
xx-en 27.22, 27.42, 27.21, 27.41, 27.46, 27.60, 27.41
en-xx 21.76, 21.83, 21.74, 21.87, 21.89, 22.01, 21.87
MethodMany-to-One One-to-Many
xx-en en-xx
Low Med. High Avg WR Low Med. High Avg WR
MLE 21.28 29.96 31.85 26.53 - 14.92 22.52 29.42 21.27 -
mixup +0.95 +0.28 +0.05 +0.52 93.33 +0.49 -0.46 -0.26 +0.05 46.66
mXEncDec -A +0.50 +0.44 +0.30 +0.42 86.67 +0.51 +0.06 +0.17 +0.31 80.00
+Hard - - - - - +0.47 + 0.08 +0.31 +0.34 86.66
mXEncDec -S + 1.76 +0.62 +0.36 +1.06 93.33 +0.45 -0.25 -0.04 +0.15 73.33
+Hard - - - - - +0.78 -0.05 + 0.35 +0.47 86.66
way test sets in FLORES-101 (Goyal et al., 2021)
to analyze the trained multilingual models.
To mitigate the data imbalance in the WMT
multilingual corpus, we follow Arivazhagan et al.
(2019b) and adopt a temperature-based data sam-
pling strategy to over-sample the low-resource lan-
guages where the temperature is set to 5. We apply
SentencePiece (Kudo and Richardson, 2018) to
learn a vocabulary of 64ksub-words. We perform
experiments in three settings: many-to-one, one-to-
many and many-to-many translations. The 15test
language pairs are cast into three groups according
to their data size: High ( >10M, 5 languages),
Low (<1M, 7) and Medium ( >1M&<10M,
3). We report not only the average detokenized
BLEU scores for each group as calculated by the
SacreBLEU script (Post, 2018) but also winning
ratio (WR) indicating the ratio of all the test sets
on which our approach beats the baseline method.
Models and Hyperparamters . Following Chen
et al. (2018), we select the Transformer Big (6
layer, 1024 model dimension, 8192 hidden dimen-
sion) as the backbone model and implement them
with the open-source Lingvo (Shen et al., 2019).
Adafactor (Shazeer and Stern, 2018) is adapted as
our training optimizer, in which the learning rate
is set to 3:0and adjusted with 40kwarm-up steps.We use a beam size of 4and a length penalty of
0:6for all the test sets. We apply language-speciﬁc
embeddings to both many-to-one and one-to-many
models while languages in many-to-many models
are speciﬁed with language tags. Many-to-one and
one-to-many models are optimized for 150ksteps
while many-to-many models run for 300ksteps.
All Transformer models utilize a large batch of
around 560064tokens over 64 TPUv4/TPUv3
chips. We average the last 8checkpoints to re-
port model performance. We tune pover the set:
f0:10;0:15;0:25;0:50gand set it to 0:15except
for many-to-one using 0:25. The temperature 
used in Eq. (10) to sample the shufﬂe ratio is se-
lected over the setf0;0:4;0:8;2:0g.= 0:8
is selected for many-to-many models while = 0
is for others as Table 1 suggests. The parameter 
in label co-reﬁnement is annealed from 0to0:7in
the ﬁrst 40Ksteps. We ﬁnd that a non-zero and
non-onecan not only better capture informative
label but also substantially improve the training
stability.
Training Efﬁciency . If we adopt the simpliﬁed tar-
get interpolation, the loss computations for L()
andL()in Eq. (13) are totally independent. But
we have to halve the batch size to load interpola-
tion examples (L()) into memory. To make the4096MethodMany-to-Many
xx-en en-xx
Low Med. High Avg WR Low Med. High Avg WR
MLE 23.2 29.02 31.19 27.03 - 15.86 22.34 29.49 21.70 -
mixup +0.79 -0.11 -0.12 +0.31 60.00 +0.32 -0.28 -0.48 -0.06 33.33
mXEncDec -A +0.88 +0.28 +0.31 +0.57 93.33 +0.64 - 0.01 +0.04 +0.31 73.33
= 0 +0.88 +0.20 -0.22 +0.38 73.33 +0.58 -0.14 -0.22 +0.17 66.66
+Hard +0.92 +0.30 +0.16 +0.54 100 +0.52 -0.20 -0.14 +0.15 66.66
mXEncDec -S +0.62 +0.34 +0.27 +0.45 86.66 +0.45 -0.10 +0.18 +0.25 60.00
= 0 +0.87 +0.06 -0.10 +0.38 66.66 +0.43 -0.40 -0.29 +0.02 37.50
+Hard + 1.78 +0.35 +0.71 +1.13 100 +0.66 -0.14 + 0.53 +0.46 60.00
MethodMany-to-Many
WMT FLORES
de!fr fr!de de!cs cs!de de!fr fr!de de!cs cs!de Avg
MLE 16.84 16.50 6.52 10.65 15.30 9.94 5.18 10.94 11.48
mixup +2.66 +1.02 -3.35 +1.01 +2.16 +0.18 -2.61 +0.95 +0.25
mXEncDec -A +3.70 +1.45 +2.33 +4.07 +2.54 +0.83 +1.82 +4.14 +2.61
+Hard + 4.98 +3.66 + 5.53 +4.36 +5.02 +2.99 + 5.11 +4.28 + 4.49
mXEncDec -S +4.94 +3.50 +0.18 + 5.31 +5.26 +3.30 -0.26 + 4.56 +3.34
+Hard +3.45 + 3.82 +3.50 +3.52 +2.46 +2.98 +3.44 +3.76 +3.37
baseline models and our models observe the same
amount of parallel examples per step, we double
the number of TPUs to compensate for it.
4.1 Main Results
We validate two variants of mXEncDec on many-
to-one, one-to-many and many-to-many settings:
•mXEncDec -A: the target interpolation tis
computed by normalizing attention in Eq. (6).
•mXEncDec -S: the target interpolation tis sim-
pliﬁed to a constant vector in Eq. (9).
We compare mXEncDec to the baseline methods:
•MLE: the vanilla Multilingual NMT is trained
with maximum likelihood estimation.
•mixup : we adapt mixup (Zhang et al., 2018)
to multilingual NMT by mixing source and
target sequences following the methods pro-
posed in Cheng et al. (2020) and Guo et al.
(2020). For a fair comparison, we also mix
co-reﬁned labels rather than one-hot labels.
Table 2 shows results on the WMT multilingual
dataset for many-to-one and one-to-many models.The comparisons between the baseline MLE and
our approach suggest that mXEncDec can improve
the translation performance on both xx-en and en-
xx translation settings (up to +1:06BLEU & 93:33
WR on xx-en and +0:47BLEU & 86:66WR on en-
xx). In particular, using simpliﬁed target interpola-
tion to substitute the noisy attention-based interpo-
lation ( mXEncDec -S vs. mXEncDec -A) can achieve
better results on xx-en translations (+0.64 BLEU)
while slightly performing worse on en-xx transla-
tions (-0.16 BLEU). After incorporating quantized
target interpolation, it yields an additional improve-
ment for mXEncDec -S on en-xx translations (+0.32
BLEU). The improvement differences between xx-
en and en-xx (+1.06 BLEU vs. +0.47 BLEU) to
some extent imply that interpolations on the target
side are more favourable to similar languages, and
interpolations on the encoder side are not sensitive
to language types.
Table 3 shows results for many-to-many models.
Among all the training methods, our approaches
still obtain the best results for both xx-en and en-4097
xx translations (up to +1:13BLEU & 100 WR
on xx-en and +0:46BLEU &73.33 WR). We
consistently ﬁnd that mXEncDec -S beneﬁts much
more from the quantized target interpolation with
+0.68 BLEU on xx-en and +0.21 BLEU on en-
xx. Although this technique slightly impairs the
performance of mXEncDec -A on both xx-en and
en-xx translations, it signiﬁcantly boosts its zero-
shot translations as shown in Table 4. We also
observe that removing the pairwise sampling with
= 0 has big negative effects on high-resource
language pairs for many-to-many models. Pairwise
sampling can not only stabilize the performance
on low-resource language pairs and signiﬁcantly
improve high-resource language pairs.
Compared to mixup , our approaches still attain
better performance except that mXEncDec -A on xx-
en performs slightly worse. mixup trains models on
linear interpolations of examples and their labels.
By contrast, mXEncDec combines training exam-
ples in a non-linear way on the source side, and
encourages the decoder to decouple the non-linear
interpolation with a ratio related to the source end.
4.2 Zero-shot Translation
To further verify cross-lingual transfer of our ap-
proaches, we utilize many-to-many models to de-
code language pairs not pesent in the training data,
i.e., zero-shot sets from WMT and FLORES. In
Table 4, our approaches achieve notable improve-
ments across all the test sets compared to base-
line methods. On average, our best approach ( mX-
EncDec -A + Hard) can gain up to +4:49BLEU
over MLE. Interestingly, this model is not the best
on general translations but delivers the best results
on zero-shot translations. These substantial im-
provements demonstrate the strong transferability
of our approaches.4.3 Multilingual Robustness
We construct a noisy test set comprising code-
switching noise to test the robustness of multi-
lingual NMT models (Belinkov and Bisk, 2018;
Cheng et al., 2019). Following the method pro-
posed in Cheng et al. (2021), we randomly replace
a certain ratio of English/non-English source words
with non-English/English target words by resorting
to an English-centric dictionary. From results in
Figure 2, we ﬁnd our approaches to exhibit higher
robustness with larger improvements as the noise
fraction increases. mXEncDec -A shows similar ro-
bustness to mXEncDec -Son zero-shot translations
and even higher robustness on xx-en translations
although its performance on clean test sets falls be-
hind mXEncDec -S.mXEncDec -Sperforms sig-
niﬁcantly better on en-xx translations compared to
other approaches. Moreover, it is noteworthy that
our approaches have better stability on xx-en trans-
lations where we replace non-English words with
English counterparts, which is in complete agree-
ment with the ﬁnding in section 4.4 that English
representations tend to be fused into non-English
representations by virtue of our approaches.
4.4 Representation Analyses
To better interpret the advantages of our approaches
over baselines, we attempt to delve deep into the
representations incurred by models. A common
method is to study the encoder representations of
multilingual NMT models (Kudugunta et al., 2019),
which we follow. We aggregate the sentence repre-
sentations by averaging the encoder outputs. The
data computing representations come from FLO-
RES (Goyal et al., 2021) as it provides a high
quality of multi-way translations implying that sen-
tences from each language are semantically equiva-
lent to each other. We use the ﬁrst 100 sentences in4098
each language to visualize representations.
We argue that the encoder in a good multilingual
NMT model prefers to distribute sentence represen-
tations based on their semantic similarities rather
than language families. Figure 3 depicts visualisa-
tions of representations plotted by t-SNE (Van der
Maaten and Hinton, 2008) on xx-en translations.
We make the following observations:
1.In each ﬁgure, sentences with the same seman-
tics incline to form a single cluster.
2.For MLE in Figure (a), most sentences are
dispersed into each cluster based on semantics
while extremely low-resource languages (Hi,
Gu, Kk) and English possess their own distinct
clusters.
3.Formixup ,mXEncDec -A and mXEncDec -S
in Figure (b)-(d), sentences from extremely
low-resource languages start to be assimilated
into their own semantic clusters.
4.FormXEncDec -A and mXEncDec -Sin Fig-
ure (c)-(d), English sentences attempt to fuse
into representations of other languages.English sentences prefer to become an individ-
ual cluster. Because when using the language tag
“<2en>” to compute English encoder representa-
tions, it is treated as a copy task instead of transla-
tion tasks for computing representations of other
languages. However, our approach promotes En-
glish sentences to be closer to their semantic equiv-
alents in other languages. This leads to enhanced
robustness toward code-switching noise when trans-
lating sentences in languages that are mixed with
English codes. The evident representation amelio-
ration for extremely low-resource languages cor-
roborates signiﬁcant BLEU improvements on low-
resource translations in Table 2 and Table 3. The
encoder learned by our approach performs the best
and complies with our argument. We also conduct
quantitative analyses to evaluate the clustering ef-
fect of each method in Figure 3. In Table 5, we
adopt three clustering metrics, SC (Silhouette Co-
efﬁcient), CH (Calinski-Harabaz Index), and DB4099Method SC " CH"DB#
MLE 0.1625 15.02 1.896
mixup 0.1821 16.56 1.796
mXEncDec -A 0.1796 16.52 1.806
mXEncDec -S0.1924 18.38 1.739
(Davies-Bouldin Index). Although these metrics
cannot adequately assess multilingual representa-
tions as they advocate distinct separations between
different clusters and tight closeness within the
same cluster, we believe they can still measure the
within-cluster variance in part. Among them, mX-
EncDec -Sperforms the best while mixup andmX-
EncDec -A yield similar performance.
5 Related Work
Multilingual NMT has made tremendous progress
in recent years (Dong et al., 2015; Firat et al.,
2016a; Johnson et al., 2017; Arivazhagan et al.,
2019b; Fan et al., 2021). Recent research ef-
forts to improve the generalization of multilingual
models concentrate on enlarging the model capac-
ity (Huang et al., 2019; Zhang et al., 2020; Lep-
ikhin et al., 2020), incorporating hundreds of lan-
guages (Fan et al., 2021), pretraining multilingual
models (Liu et al., 2020), and introducing addi-
tional regularization constraints (Arivazhagan et al.,
2019a; Al-Shedivat and Parikh, 2019; Yang et al.,
2021). Our work is related to the last three ones
in that they try to enable models to better transfer
across languages by introducing an alignment loss
to learn an interlingua (Arivazhagan et al., 2019a)
or imposing an agreement loss on translation equiv-
alents (Al-Shedivat and Parikh, 2019; Yang et al.,
2021). However, we propose to utilize mXEncDec
to directly combine language pairs for better ex-
ploitation of cross-lingual signals.
Another related research line is data mixing.
Since mixup (Zhang et al., 2018; Yun et al., 2019)
was proposed in computer vision, we have observed
great success in NLP (Guo et al., 2019; Cheng et al.,
2020; Guo et al., 2020; Cheng et al., 2021). mX-
EncDec shares the commonality of combining ex-
ample pairs as inspired by XEncDec (Cheng et al.,
2021). To the best of our knowledge, we are theﬁrst to fuse different language pairs to improve
cross-lingual generalization and robustness for mul-
tilingual NMT.
6 Conclusion
We have presented mXEncDec to fuse different
language pairs at instance level for multilingual
NMT, which enables the model to better exploit
cross-lingual signals. Experimental results on gen-
eral, zero-shot and noisy test sets demonstrate that
our approach can signiﬁcantly improve the cross-
lingual generalization, zero-shot transfer and ro-
bustness of multilingual NMT models. Representa-
tion analyses further conﬁrms that our approach
is capable of learning better multilingual repre-
sentations, which coincides with improvements
in BLEU. We plan to investigate whether this ap-
proach can improve the model generalization in a
broader scope like domain generalization. We ﬁnd
thatmXEncDec can easily achieve notable improve-
ments for xx-en translations because they share an
identical target language. However, there still exits
huge headroom for en-xx translations. We plan to
explore how to interpolate target languages more
effectively, for example, possibly considering lan-
guage similarity.
References410041014102