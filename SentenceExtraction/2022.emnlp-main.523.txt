
Zeqi Tan, Yongliang Shen, Xuming Hu, Wenqi Zhang, Xiaoxia Cheng,
Weiming Lu,Yueting ZhuangZhejiang University,Tsinghua University{zqtan,syl,luwm,yzhuang}@zju.edu.cn{hxm19}@mails.tsinghua.edu.cn
Abstract
Joint entity and relation extraction has been a
core task in the field of information extraction.
Recent approaches usually consider the extrac-
tion of relational triples from a stereoscopic
perspective, either learning a relation-specific
tagger or separate classifiers for each relation
type. However, they still suffer from error prop-
agation, relation redundancy and lack of high-
level connections between triples. To address
these issues, we propose a novel query-based
approach to construct instance-level represen-
tations for relational triples. By metric-based
comparison between query embeddings and to-
ken embeddings, we can extract all types of
triples in one step, thus eliminating the error
propagation problem. In addition, we learn
the instance-level representation of relational
triples via contrastive learning. In this way, re-
lational triples can not only enclose rich class-
level semantics but also access to high-order
global connections. Experimental results show
that our proposed method achieves the state of
the art on five widely used benchmarks.
1 Introduction
Extracting structured information from open do-
main texts is a long-standing study topic in NLP.
Joint entity and relation extraction aims to mine
high-quality relational triples ( subject, relation, ob-
ject) from unstructured texts. For example, in the
sentence “900 people cross the border between
Malaysia and Singapore”, the subject entity 900
people and the object entity border have a physical
location relation PHYS .
The current entity and relation extraction meth-
ods can be divided into two categories: pipeline
methods (Chan and Roth, 2011; Lin et al., 2016)
and joint methods (Miwa and Bansal, 2016; Kati-
yar and Cardie, 2017). Despite the flexibility of
pipeline methods, they face the error propagation
and the lack of interaction problems (Katiyar andFigure 1: (a) H represents sentence representa-
tion, R represents potential relation type em-
beddings. PRGC first filters out unlikely relations in
a sentence, and then learns the relation-specific tag-
ger for potential relations. (b) The gray circle below
represents the initial state of query embeddings which
are type-agnostic, and the star above represents rela-
tion type embeddings. We can break the limitation of
type-independence and learn the high-order connections
between different types via contrastive learning.
Cardie, 2017; Wang and Lu, 2020). Therefore,
many strategies have been proposed to unify the
two tasks. Zheng et al. (2017) extends tagging
schema to tag both entities and relations. However,
it cannot handle the triple overlapping problem.
To tackle this problem, the generation-based
methods (Zeng et al., 2018, 2020; Nayak and Ng,
2020; Ye et al., 2021), the table-filling methods
(Wang et al., 2020b; Wang and Lu, 2020; Wang
et al., 2021) and the cascading methods (Luan et al.,
2019; Yuan et al., 2020; Wei et al., 2020; Zheng
et al., 2021) have been investigated.
Recently, an increasing number of studies have
come to consider the relational triple extraction task
from a stereoscopic perspective. These approaches
treat relation type as an important element, and
either learn a distinct classifier (Liu et al., 2020;
Wang et al., 2020b) for each relation type or con-
struct a relation-specific sequence tagger (Yuan
et al., 2020; Wei et al., 2020; Zheng et al., 2021).
In this way, these methods can better decouple the
task-specific features for relations and entities, and7677yield promising results. However, they still have
some weaknesses. First , error propagation is a
well-known problem. Wei et al. (2020) first identi-
fies the subject and then extracts the corresponding
triples. However, there are inevitable errors in the
first stage. PRGC (Zheng et al., 2021), as shown
in Figure 1 (a), first picks the potential relations
in the sentence, and later tags the relation-specific
entities. The correct relations are then likely to be
filtered out in this process. Second , these methods
mostly suffer from the relation redundancy prob-
lem. Wei et al. (2020); Wang et al. (2020b) assem-
bles triples on all relation types, which generates
numerous invalid operations, leading to sparse label
and low convergence rate. Third , these methods
process the triples for each relation type indepen-
dently, ignoring the high-level global connections
between different types of triples. For example, if
the triple ( Maryland, country, U.S. ) has been rec-
ognized, then U.S. is not supposed to appear in any
other family relationship, such as spouse of .
To tackle the above problems, we propose a
novel query-based instance discrimination network
(QIDN), in which we leverage instance query to
construct instance-level representation for different
types of relational triples. As in Figure 1 (b), we
first use a set of type-agnostic query embeddings
to obtain useful contextual information from a pre-
trained language model. Afterwards, by the metric-
based comparison between query embeddings and
token embeddings, we can extract triples of all
types in one step. In this manner, the error propa-
gation problem is eliminated. Moreover, since we
query the most plausible relation type directly with
the query vector, the problem of relation redun-
dancy is well mitigated. In addition, we construct
instance-level representations for relational triples
by contrastive learning. Specifically, as shown in
Figure 1 (b), we set two training objectives: (1)
Intra-class instance pairs should get higher sim-
ilarity than inter-class ones. (2) Instance repre-
sentations should be closer to their corresponding
relation type representations. We aim to break the
limitation of type-independence in previous meth-
ods and establish the global connection between
triples.
Our main contributions are as follows:
•We tackle the relational triple extraction task
from a novel perspective, focusing on con-
structing instance-level representations for re-
lational triples. Based on a simple similaritymeasure, we extract all types of triples in one
step, thus avoiding cascading errors.
•We learn the instance-level representation of
relational triples by contrastive learning. In
this way, relational triples are not only able to
establish high-order global connections, but
also to enclose rich class-level semantics.
•Extensive experiments on five public bench-
marks demonstrate that our model achieves
state-of-the-art, and significantly outperforms
a range of robust baselines.
2 Related Work
2.1 Mainstream methods
Relational triple extraction can be solved with two
different models. Assuming that the entities in a
text sequence are obtained by NER model (Zhang
et al., 2021; Tan et al., 2021; Shen et al., 2022;
Zhang et al., 2022), relation extraction can be con-
sidered as a classification task (Zeng et al., 2014;
Wang et al., 2019; Hu et al., 2020, 2021). Since
these methods require additional entity annotators
to carry out a pipeline, they generally face the error
propagation problem and the lack of interaction
between tasks (Lin et al., 2016; Shen et al., 2021a).
Recently, Zhong and Chen (2021) propose a simple
pipeline approach reaching state-of-the-art. They
re-construct the input text of the relation model
with the entity recognition results, delivering the
type and location info of entities in this manner.
Beside the pipeline methods, the joint methods
can be divided into the generation-based methods
(Zeng et al., 2018, 2020; Nayak and Ng, 2020; Ye
et al., 2021), the table-filling methods (Gupta et al.,
2016; Zhang et al., 2017; Wang et al., 2020b, 2021)
and the cascading methods (Luan et al., 2019; Yuan
et al., 2020; Wei et al., 2020; Zheng et al., 2021).
The generation-based methods use a sequence-to-
sequence model to generate the relation triples di-
rectly from the sentence, while the table-filling
methods treat on/off-diagonal entries as labels for
entities and relationships, respectively. Differently,
Luan et al. (2019) treats entity and relation ex-
traction as a cascading two-level span classifica-
tion task. Later, some work introduce a relation-
specific perspective to treat triple extraction as a
relation-first (Yuan et al., 2020; Zheng et al., 2021)
or relation-middle (Wei et al., 2020) cascading pro-
cess. Compared to them, our method can form
triples in one step and avoid the cascading errors.76782.2 Query-based methods
To exploit the well developed machine reading com-
prehension models, a number of works (Li et al.,
2019, 2020; Zhao et al., 2020; Du and Cardie, 2020)
extend the MRC paradigm to information extrac-
tion tasks. Li et al. (2019) recast the entity and
relation extraction task as a multi-turn question an-
swering problem. Their queries are constructed
based on pre-defined templates and the answers
are the entity spans in the sentence. Zhao et al.
(2020) explores to cover more relational semantics
with more handcrafted query templates based on
subcategories of relations.
Different from the above methods where queries
are manually constructed, in the field of object
detection in computer vision, there is a range of
work (Carion et al., 2020; Zhu et al., 2020; Sun
et al., 2020) that uses trainable query to learn task-
specific features automatically. DETR (Carion
et al., 2020) successfully integrates Transformer
(Vaswani et al., 2017), originally designed for text,
by using a set of learnable queries. These queries
are type-agnostic and have the potential to build
global dependencies between objects at diverse
scales. Inspired by this, we employ such query
embeddings to construct instance-level represen-
tations for relational triples and establish global
connections between different types of triples.
2.3 Contrastive relation learning
MTB (Soares et al., 2019) extracts relation-aware
semantics from text by comparing sentences with
the same entity pairs. Extended MTB, CP (Peng
et al., 2020) samples relational triples with better
diversity and increases the coverage of entity types
and different contexts. Different from the sentence-
level relational contrastive learning (Soares et al.,
2019; Peng et al., 2020; Liu et al., 2022), ERICA
(Qin et al., 2021) proposes a relation discrimina-
tion task to distinguish whether two relation types
are semantically similar or not, better considering
the interaction between multiple relations. Com-
pared to them, we use query embeddings to con-
struct instance-level triple representations, taking
into account both entity features and relation fea-
tures, which may motivate new pre-training tasks.
3 Method
3.1 Task Formulation
The entity and relation extraction task aims to
extract a set of entities and a set of relationsfrom the text. Formally, given an input sentence
X=x, x, . . . , x(xis the i-th token, nis the
sentence length), an entity in the entity set Eis
denoted as (x, x, t), where xandxare the left
and right token of the entity with a pre-defined en-
tity type tinY. For the relation set R, a relation
is denoted as (e, e, t), where e, e∈ Eare the
subject and object entities and tis a pre-defined
relation type in Y. Besides, we add an additional
label∅toYandYto indicate that no entity or
no relation is recognized.
3.2 Sentence Encoder
As shown in Figure 2, given the input sentence X,
we first use the pre-trained BERT model (Devlin
et al., 2019) to obtain a contextual representation
for each token. Afterwards, to better consider the
order information of the tokens, we feed the token
representations into BiLSTM (Zhang et al., 2015)
to get the final sentence representation H∈R,
where nis sentence length and dis hidden size.
3.3 Triple Prediction
In our method, we employ a set of learnable in-
stance queries same as DETR (Carion et al., 2020),
which are denoted as Q=R. Each query
(denoted as a vector of size d) is responsible for
extracting one relational triple. These queries are
randomly initialized and the number of the queries
Mis pre-specified. Different from DETR, in or-
der to distinguish entity and relation task-specific
features, we project the queries Qinto entity and re-
lation branches by FFN before entering the decoder.
After decoding, we use several prediction heads to
map these query vectors to different representation
spaces for metric-based comparison with the token
embeddings. In this way, all types of triples can be
recognized in one step.
Transformer-based Decoder The decoder is
composed of a stack of Ltransformer (Vaswani
et al., 2017) layers, and the decoding process
mainly involves the multi-head attention mecha-
nism. For simplicity, we denote the attention as:
Attention (Q, K, V ) = softmax/parenleftbiggQK
√d/parenrightbigg
V,
(1)
where Q,K,Vare the query, key and value matrix
respectively, and the 1/√dis the scaling factor.
In the cross-attention mechanism module, we do
not directly take Hfrom the sentence encoder as7679
key and value, instead, we construct span-level
representations to obtain hierarchical semantic in-
formation. Let S=s, s, . . . , sbe all possible
spans in sentence Xof up to limited length ( nis
the number of spans). Give a span s∈S, the span
representation His defined as:
H=/bracketleftbig
H ;H ;ϕ(s)/bracketrightbig
,(2)
where [; ]denotes concatenation operation, H
andH are the representations of the bound-
ary tokens. ϕ(s)denotes the span-length feature
(Zhong and Chen, 2021). Then, the span-level
representations is denoted as H∈R.
Through the Ldecoding layers, instance queries Q
are decoded into QandQas:
[Q;Q] =Decoder ([QW;QW], H),
(3)
where W, W∈Rare trainable parameters,
Q, Q∈Rdenote relation and entity queries.
For a more intuitive understanding, we illustrate
the architecture of the decoder in Appendix B.
Relation Head For relation queries Q, we feed
them into a single layer of FFN to predict the cate-
gories of their corresponding triples. Formally, wedefine the probability of the i-th query belonging
to type cas:
P=exp(QW+b)
/summationtextexp(QW+b), (4)
where W∈Randb∈Rare trainable
parameters.
Entity Head To predict the boundary tokens of
triples, we first perform linear projections for the
entity queries Qand the token representations H
by a single layer of FFN as:
E=QW, H=HW, (5)
where δ∈ C={l, r, l, r}denotes the
left or right boundaries of subject or object entities
andW, W∈Rare projection parameters. To
measure the similarity between them, we adopt
cosine similarity function S(·)as:
S(v,v) =v
∥v∥·v
∥v∥. (6)
Then, we can calculate the probability that the
j-th token is the boundary token of the i-th entity7680queries as:
P=expS/parenleftig
E, H/parenrightig
/summationtextexpS/parenleftig
E, H/parenrightig, (7)
where nis the number of tokens in the sentence.
Finally, based on the probability PandP(δ∈
C), we can predict all types of triples. Besides,
for corpus with labeled entity types, we add two
additional entity type heads like the relation head.
3.4 Instance Discriminator
In this module, we first aggregate several repre-
sentations from the prediction heads to form the
initial representation of triples, and then we set up
two contrastive learning objectives to build global
connections between triple instances and learn rich
class-level semantic information.
Aggregation To get the initial representation of
triples, we first use FFN to project the relation
queries Q, aligned with Efrom the entity head.
Then, we aggregate category representations and
boundary representations by a simple summing
operation. The triple instance representation vis:
v=QW+/summationdisplayE. (8)
Training Objective With the triple instance rep-
resentation vand the similarity function defined
in equation 6, we discuss how to set our training
objectives. Denote R=/braceleftbig
r,···,r/bracerightbig
as the set
of relation type embeddings which are randomly
initialized. We expect to achieve two goals with
optimization: (1) For instance-instance pairs, intra-
class pairs should get higher similarity than inter-
class ones. (2) For instance-type pairs, instance rep-
resentations should be closer to their corresponding
relation type representations. To reach these two
goals, we define the objective function based on In-
foNCE (van den Oord et al., 2018), which is widely
applied in contrastive learning.
For the instance-instance objective, we minimize
the following loss:
L=−/summationdisplay/summationdisplaylogexpS/parenleftig
v,v/parenrightig
/summationtextexpS/parenleftig
v,v/parenrightig,
(9)
where (v,v)denotes the instance pair of the
same type c. Similarly, the instance-type loss is
defined as:
L=−/summationdisplaylogexpS(v,r)/summationtextexpS(v,r), (10)
where vis the instance of type candr∈ R is the
relation embedding corresponding to type c. We
aim to break the limitation of type-independence
and establish the global connection between differ-
ent types of triples.
3.5 Training and Inference
Training In training, we define the triple loss
Laccording to the type probability Pand the
boundary probability Pas:
L=−/summationdisplay/parenleftigg
logP+/summationdisplaylogP/parenrightigg
,(11)
where Mis the number of instance queries, σis the
optimum matching calculated as same as Carion
et al. (2020). The final loss function Lis computed
asL=L+L+L.
Inference During inference, the triple predicted
by the i-th instance query is Y=/parenleftbig
Y,Y/parenrightbig
,
δ∈ C.Y= arg max(P)is the relation type.
Y= arg max(P)are the left and right bound-
ary tokens of subject-object entities, and triples
with the predicted type of ∅will be filtered out.
4 Experiments
4.1 Experimental Setup
Datasets We conduct our experiments on four
widely used datasets: NYT (Riedel et al., 2010),
WebNLG (Zeng et al., 2018), ACE05 (Walker et al.,
2006), and SciERC (Luan et al., 2018). NYT
(Riedel et al., 2010) is sampled from New York
Times news articles and annotated by distant su-
pervision. NYTis another version of it which
annotates the whole span of entities. WebNLG is
originally created for Natural Language Genera-
tion (NLG) and is adopted by (Zeng et al., 2018)7681
as a relation extraction dataset. ACE05 corpora
are collected from a wide range of domains, such
as newswire and online forums. SciERC includes
annotations for scientific entities, their relations,
and coreference clusters for 500 scientific abstracts.
Table 1 shows the detailed dataset statistics.
Evaluation Metrics Following previous works
(Wei et al., 2020; Zheng et al., 2021), we adopt
micro Precision (Prec.), Recall (Rec.) and F1-score
(F1) on NYT/WebNLG under partial match, on
NYTunder exact match. For ACE05 and Sci-
ERC, same as (Zhong and Chen, 2021; Yan et al.,
2021), we adopt the micro F1-score as the evalua-
tion metric for both NER and RE. We use the strict
evaluation criterion that a relation is correct only
if the relation type is correct and the type as well
as boundaries of its corresponding subject-object
entities are correct.
Implementation Details For fair comparison
with prior work, we use bert-base-cased (Devlin
et al., 2019) for NYT/WebNLG, albert-xxlarge-v1
(Lan et al., 2020) for ACE05, and scibert-scivocab-
uncased (Beltagy et al., 2019) for SciERC in the
sentence encoder. The instance queries and rela-
tion embeddings are randomly initialized with the
normal distribution N(0.0,0.02)and we design a
comparison experiment in Appendix A to deter-
mine the query number Mas 15. The number
of the BiLSTM layer is 3 and the number of our
decoder layer Lis set to 5. The span enumera-
tion length is set to 8. We adopt a batch size of 8
on NYT and ACE05, and 4 on WebNLG and Sci-
ERC in training. We use the AdamW (Loshchilov
and Hutter, 2017) optimizer with a linear warmup-
decay learning rate schedule. The peak learning
rate is set to 1e-5 for the pre-trained model and
3e-5 for the other parameters. We train each model
for 100 epochs with a dropout of 0.1 (Srivastava
et al., 2014).
4.2 Overall Performance
Table 2 shows the overall performance of our pro-
posed method (QIDN) as well as the baseline mod-
els on the NYT and WebNLG datasets. Overall,
our method outperforms all the baseline model
consistently, and achieves the new state-of-the-art.
Compared with the robust baseline models Cas-
Rel (Wei et al., 2020) and TPLinker (Wang et al.,
2020b), our model makes a significant improve-
ment of +2.1% and +2.0% in absolute F1-measure7682
on WebNLG. Compared to the best cascade method
PRGC (Zheng et al., 2021), our method achieves
consistent improvement in terms of all three eval-
uation metrics on WebNLG. Similarly, Table 3 in-
dicates that our method reaches state-of-the-art on
both the NER and RE tasks for ACE05 and Sci-
ERC. In comparison to the best joint method (Yan
et al., 2021), we achieve superior performance in
F1-measure on ACE05 and SciERC (+1.5% and
+3.0% for NER, +1.4% and +1.1% for RE). The ex-
perimental results demonstrate the effectiveness of
our proposed method on the joint entity and relation
extraction task. We believe the main improvement
comes from the fact that our approach avoids cas-
cading errors and the relation redundancy problem.
The cascading methods (Wei et al., 2020; Zheng
et al., 2021) inevitably suffer from cascading errors.
And learning separate classifiers for each relation
type (Wang et al., 2020b, 2021) causes the prob-
lem of relation redundancy. Our approach solves
these problems simultaneously and thus achieves
promising results.
4.3 Analysis on Complex Scenarios
Following Wei et al. (2020); Wang et al. (2020b);
Zheng et al. (2021), we evaluate our model on
different triple overlapping patterns and different
triple numbers. The triple overlapping problem
refers to triples sharing the same single entity (SEO,
i.e. SingleEntityOverlap) or entity pair (EPO, i.e.
EntityPairOverlap). For example, In “Alice and
Joe were born in the US”, triples ( Alice, birthplace,
USA) and ( Joe, birthplace, USA ) share the single
entity USA, while triples ( Alice, birthplace, USA )
and ( Alice, residence, USA ) share the whole pair.
The detailed statistic is described in Appendix.
As shown in Table 4, we achieve the best per-
formance on all three overlapping patterns on
WebNLG and NYT. In the normal set, we achieve
+1.1% improvement in F1-measure on WebNLG,
while on NYT the improvement is very slight. We
argue that this is because NYT is generated with
distant supervision, and annotations are often in-
complete, especially for the normal pattern. In
addition, as in Table 4, our model performs well
for both datasets with different number of triples,
especially for the sentences with more than 5 triples
(+1.3% on NYT and +1.4% on WebNLG). Com-
pared to previous methods, our approach can break
the limitation of type-independence and establish
the global connection between different triples, and
thus be more robust than baselines when dealing
with these challenging complex scenarios.
4.4 Ablation Study
In this section, we take a closer look at the mod-
ules in our model that contribute to performance
with five settings. (1) w/oH: replace the span-
level representation with the token-level represen-
tation, (2) w/oQ, Q: remove entity and relation
query branches and use only instance queries, (3)
w/oL: remove the training objective between
instance pairs, (4) w/oL: remove the training ob-
jective between instances and relation embeddings,
(5)w/oL,L: remove both contrastive learning
training objectives.
From Table 5 we observe that w/oL,L
leads to the most significant performance decrease
in absolute F1-mearsure (-1.2% on NYT and -1.4%
on WebNLG) and removing any of them causes7683visible performance drops, which demonstrates the
effectiveness of the two contrastive learning objec-
tives we set. In addition, as show in Table 5, con-
structing span-level sentence representations brings
performance gains (+0.2% on NYT and +0.6% on
WebNLG), which indicates that span-level repre-
sentations contain structured information that is
more useful for triple extraction than token-level
representations. Similarly, dividing the instance
queries into entity and relation branches to facili-
tate the distinction between their task-specific char-
acteristics, delivering performance improvement.
We provide a more detailed analysis for entity and
relation branches in Appendix B.
5 Topology of Relations
To illustrate that our approach establishes connec-
tions between different types of relations, we visu-
alize the relation representation learned on NYT.
We first perform L2 normalization on the relation
embeddings and then use PCA (Abdi and Williams,
2010) to reduce their dimension. We filter out 5
long-tail relations that appear less than 40 times in
the entire training set and then classify them into
4 broad categories based on label prefixes in NYT.
As shown in Figure 3, the topology between the re-
lations indicates the semantic connections between
them. For example, the two types of sport relations
almost overlap, while the six location relations (in-
cluding country ,capital , etc.) are all scattered in
the lower left corner. However, we observe an ex-
ception on “People”, where the 6 subcategories
are not spread together spatially. We attribute this
to semantic divergence. The three subcategories(place_lived ,place_of_birth andplace_of_death )
semantically express position information, which
is distinct from the other subcategories. These all
demonstrate that our method can break the limits
of type-independence and well consider the high-
order connections between different relation types.
Besides, we conduct a detailed case study to show
the connections between queries in Appendix C.
6 Error Analysis
In this section, we analyze the different error pro-
portions on two tasks: entity classification error
(ECE) and entity location error (ELE) for NER,
relation classification error (RCE), entity-pair clas-
sification error (PCE), and entity-pair location error
(PLE) for RE. On the NER task, we can observe
from Figure 4 that the proportion of classification
errors and localization errors for entity queries on
ACE05 is comparable. But on SCiERC, the local-
ization errors are more pronounced. We argue that
on account of the longer average length of scientific
term entities in SciERC, the identification of entity
boundaries becomes more difficult.
As shown in Figure 4, identifying the boundary
tokens of the subject and object entities is the most
difficult on both ACE05 and SciERC. We argue that
this is due to the complicated patterns of nesting
as well as overlapping in boundary recognition. In
addition, the proportion of relation classification
errors is minor on both datasets, which indicates
the ability of queries in categorization.
7 Conclusion
In this paper, we propose a novel query-based in-
stance discrimination network for relational triple7684extraction. Based on instance queries, we can ex-
tract all types of triples in one step, thus avoiding
the problem of error accumulation and relation re-
dundancy. Besides, we set two training objectives
for contrastive learning to establish high-order con-
nections between triples while learning rich class-
level semantic information. Experimental verifica-
tions on five datasets demonstrate that the proposed
method reaches state-of-the-art.
Limitations
We discuss here the limitations of the method in
this paper. First, this method requires specifying
the relation type in advance and cannot handle un-
seen categories, which is weak for relation extrac-
tion in open domains. Second, due to the inherent
length limit of BERT, this model cannot deal with
excessively long texts. Besides, for the case of a
single entity corresponding to multiple mentions in
a long document, it can pose a great challenge to
the proposed method. Finally, incorporating prior
knowledge into query embeddings is a promising
direction for optimization.
Acknowledgments
This work is supported by the National Key Re-
search and Development Project of China (No.
2018AAA0101900), the Key Research and Devel-
opment Program of Zhejiang Province, China (No.
2021C01013), CKCEST, and MOE Engineering
Research Center of Digital Library.
References7685768676877688A Number of queries
Since the number of queries is pre-fixed, we con-
duct a comparison experiment in Table 6 to find
the suitable setting. We employ the experiment on
SciERC with a maximum number of 10 triples in
one sentence. We observe that when the number is
obviously larger than the ground truth, the perfor-
mance of the model does decrease. We argue that
this is due to the imbalance between positive and
negative samples, as redundant queries predict the
none category ∅. Eventually, the query number M
in our experiments is set to 15, which is deployed
on all the other datasets.
B Analysis of query branches
In this section, we explore the entity and relation
query branches in our decoder, the detailed archi-
tecture is shown in Figure 5. Based on different
attention mask matrices for queries, there are three
settings. (1) w/o ent-rel : entity queries are not
visible to relation queries. (2) w/o rel-ent : relation
queries are not visible to entity queries. (3) w/o
both : entity queries and relation queries are not
visible to each other. As shown in Table 7, if entity
queries are visible to relation queries, the perfor-
mance of the model on RE will improve by +1.1%
(from 38.2% to 39.3%). Correspondingly, if rela-
tion queries are visible to entity queries, the model
performance on NER will increase by +0.7% (from
68.9% to 69.6%). Further, if they are visible to
each other, the F1-measure of the model on NER
and RE will be improved by +1.1% and +1.4%,
respectively. We attribute this to the fact that we
can model the inherent dependencies between NER
and RE tasks through the attention between entity
queries and relation queries.
C Case study
In order to provide a more intuitive understanding
of the connection between queries, we visualize
the attention weights between entity queries and
between relation queries in the decoder with bertviz7689
(Vig, 2019). First, we analyze the connection be-
tween the queries corresponding to the gold labels
and the queries corresponding to the “None” la-
bels, and then we analyze the connection between
queries corresponding to the gold labels.
As shown in the left three columns in Figure 6,
the attentions are weighted equally between queries
before entering the decoding layers. However, as
the decoding layer deepens, the queries correspond-
ing to the gold labels will gradually neglect the
none labels. For entity queries, when the number
of decoding layers is 5, the entity queries corre-
sponding to gold entities have most of their atten-
tion focused on themselves. The same holds true
for relation queries. The observation suggests that
these queries do capture dependencies between en-
tities and between relations. And they are capable
of considering the gold and none labels separately
during decoding.
Beyond being able to distinguish between the
gold and none labels, the queries corresponding
to the gold labels have different attention weights
from each other. From the right column of Figure 6,
we can notice that the entity “Baghdad” pays more
attention to “Refugees” and “Fallujah” but ignores
“Areas” which is a LOC entity. This is due to the
fact that “Baghdad” has the relation PHYS_1 with
“Refugees”, and has the same entity type GPE as
“Fallujah”. Similarly, the relation PHYS_1 shows
more attention to PHYS_3 than to PHYS_2 , since
PHYS_1 andPHYS_3 have the same subject entity“Refugees” and the same type GPE of object entity.
It indicates that our approach can well exploit the
dependencies between triples.7690