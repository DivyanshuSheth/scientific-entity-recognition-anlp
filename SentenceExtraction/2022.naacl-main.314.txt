
Seo Yeon Park and Cornelia Caragea
Computer Science
University of Illinois Chicago
spark313@uic.edu cornelia@uic.edu
Abstract
MixUp is a data augmentation strategy where
additional samples are generated during train-
ing by combining random pairs of training
samples and their labels. However, select-
ing random pairs is not potentially an opti-
mal choice. In this work, we propose TD-
MixUp, a novel MixUp strategy that lever-
ages Training Dynamics and allows more in-
formative samples to be combined for gener-
ating new data samples. Our proposed TD-
MixUp first measures confidence, variability,
(Swayamdipta et al., 2020), and Area Under
the Margin (AUM) (Pleiss et al., 2020) to iden-
tify the characteristics of training samples (e.g.,
as easy-to-learn or ambiguous samples), and
then interpolates these characterized samples.
We empirically validate that our method not
only achieves competitive performance using
a smaller subset of the training data compared
with strong baselines, but also yields lower ex-
pected calibration error on the pre-trained lan-
guage model, BERT, on both in-domain and
out-of-domain settings in a wide range of NLP
tasks. We publicly release our code.
1 Introduction
MixUp (Zhang et al., 2018) is a simple data aug-
mentation strategy in which additional samples are
generated during training by combining random
pairs of training samples and their labels. While
simple to implement, MixUp has been shown to
improve both predictive performance and model
calibration (i.e., avoiding over-confident predic-
tions) (Guo et al., 2017) due to its regularization
effect through data augmentation (Thulasidasan
et al., 2019). However, selecting random pairs in
MixUp might not necessarily be optimal.
Despite this, MixUp has been explored for NLP
tasks with substantial success using hidden state
representations (Verma et al., 2019). For instance,
Sun et al. (2020) explored MixUp, which uses thehidden representation of BERT (Devlin et al., 2019)
to synthesize additional samples from randomly se-
lected pairs. Yin et al. (2021) proposed MixUp,
which uses the hidden representation of RoBERTa
(Liu et al., 2019) to interpolate all samples in the
same mini-batch to better cover the feature space.
To date, only a few prior works have focused on se-
lecting informative samples for MixUp. For exam-
ple, Chen et al. (2020) proposed semi-supervised
learning, which interpolates labeled and unlabeled
data based on entropy. Kong et al. (2020) explored
BERT calibration with MixUp, which generates
new samples by exploiting the distance between
samples in the feature space.
Recently, Swayamdipta et al. (2020) introduced
data maps, which allow evaluating data quality by
using training dynamics (i.e., the behavior of a
model as training progresses). Specifically, they
consider the mean and standard deviation of the
gold label probabilities, predicted for each sample
across training epochs (i.e., confidence and vari-
ability), and characterize data into three different
categories: (1) samples that the model predicts cor-
rectly and consistently (i.e., easy-to-learn ); (2)
samples where true class probabilities vary fre-
quently during training (i.e., ambiguous ); and (3)
samples that are potentially mis-labeled or erro-
neous (i.e., hard-to-learn ). The author revealed
that the easy-to-learn samples are useful for model
optimization (parameter estimation) and without
such samples the training could potentially fail to
converge, while the ambiguous samples are those
on which the model struggles the most and push the
model to become more robust, hence, these ambigu-
ous samples are the most beneficial for learning
since they are the most challenging for the model.
Inspired by these observations, we propose a
novel MixUp strategy which we call TDMixUp
that monitors training dynamics and interpolates
easy-to-learn samples with ambiguous samples in
the feature space. That is, we pair one sample from4244the easy-to-learn set with another sample from the
ambiguous set to allow more informative samples
to be combined for MixUp. Accordingly, we gen-
erate new samples that share the characteristics
of both easy-to-learn and ambiguous data samples
and are hence more beneficial for learning. How-
ever, the easy-to-learn and the ambiguous sets can
contain mis-labeled samples that can degrade the
model performance. Consequently, we measure
another training dynamic, Area Under the Margin
(AUM) (Pleiss et al., 2020), to filter out possibly
mis-labeled samples in each set. We validate our
proposed method on a wide range of natural lan-
guage understanding tasks including textual entail-
ment, paraphrase detection, and commonsense rea-
soning tasks. We achieve competitive accuracy and
low expected calibration error (Guo et al., 2017) on
both in-domain and out-of-domain settings for the
pre-trained language model BERT (Devlin et al.,
2019), without using the full training data.
2 Proposed Approach: TDMixUp
We introduce our proposed TDMixUp, which gen-
erates additional samples based on the characteris-
tics of the data samples. We first reveal the char-
acteristics of each data sample by using training
dynamics, i.e., confidence, variability, and Area
Under the Margin (AUM). We then describe our
MixUp operation that combines training samples
based on the above data characteristics that are
measured during training.
2.1 Data Samples Characterization
We first introduce confidence andvariability , that
are used to evaluate the characteristics of each indi-
vidual sample (Swayamdipta et al., 2020). These
statistics are calculated for each sample (x, y)
overEtraining epochs.
Confidence We define confidence as the mean
model probability of the true label yacross epochs:
ˆµ=1
E/summationdisplayp(y|x) (1)
where pdenotes the model’s probability with
parameter θat the end of eepoch.
Variability We define variability as the standard
deviation of pacross epochs E:
ˆσ= /summationtext(p(y|x)−ˆµ)
E(2)Given these statistics per sample, we identify the
top 33% easy-to-learn samples, i.e., those samples
that the model predicts correctly and consistently
across epochs (high-confidence, low-variability),
and the top 33% ambiguous samples, i.e., those
samples whose true class probabilities have a high
variance during training (high-variability).
Area Under the Margin (AUM) As another
measure of data quality, we monitor training dy-
namics using the Area Under the Margin (AUM)
(Pleiss et al., 2020). AUM measures how different
a true label for a sample is compared to a model’s
belief at each epoch and is calculated as the aver-
age difference between the logit values for a sam-
ple’s assigned class (gold label) and its highest non-
assigned class across training epochs. Formally,
given a sample (x, y), we compute AUM (x, y)
as the area under the margin averaged across all
training epochs E. Specifically, at some epoch
e∈E, the margin is defined as:
M(x, y) =z−max(z) (3)
where M(x, y)is the margin of sample xwith
true label y,zis the logit corresponding to the
true label y, andmax(z)is the largest other
logit corresponding to label knot equal to y. The
AUM of (x, y)across all epochs is:
AUM (x, y) =1
E/summationdisplayM(x, y) (4)
Intuitively, while both AUM and confidence mea-
sure training dynamics, confidence simply mea-
sures the probability output of the gold label and
how much it fluctuates over the training epochs. In
contrast, AUM measures the probability output of
the gold label with respect to the model’s belief in
what the label for a sample should be according
to its generalization capability (derived by observ-
ing other similar samples during training). More
precisely, AUM considers each logit value and mea-
sures how much the gold label assigned logit value
differs from the other largest logit value, which
allows identifying mis-labeled samples.
To identify possibly mis-labeled data in each set
(i.e., the set of easy-to-learn and the set of ambigu-
ous samples that are categorized by confidence and
variability as described above), we first fine-tune
a model on each set, respectively, with inserting
fake data (i.e., threshold samples). Data with simi-
lar or worse AUMs than threshold samples can be4245assumed to be mis-labeled (Pleiss et al., 2020). We
construct threshold samples by taking a subset of
the training data and re-assigning their labels ran-
domly, including a class that does not really exist.
Specifically, given Ntraining samples that belong
tocclasses, we randomly select N/(c+1) samples
per class and re-assign their labels to classes that
are different from the original class. We then train a
model on training samples including threshold sam-
ples and measure the AUMs of all training data. We
identify possible mis-labeled data by computing a
threshold value (i.e., the kth percentile threshold
sample AUMs where kis a hyper-parameter chosen
on the validation set). At last, we filter out samples
that have lower AUM than the threshold value.
2.2 MixUp
MixUp training generates vicinity training samples
according to the rule introduced in Zhang et al.
(2018):
˜x=λx+ (1−λ)x
˜y=λy+ (1−λ)y(5)
where xandxare two randomly sampled input
points, yandyare their associated one-hot en-
coded labels, and λis a mixing ratio sampled from
a Beta( α,α) distribution with a hyper-parameter
α. In standard MixUp, training data is augmented
by linearly interpolating random training samples
in the input space. In contrast, our proposed TD-
MixUp interpolates one easy-to-learn sample with
one ambiguous sample after applying AUM to filter
potential erroneous samples that harm performance.
Our current implementation uses easy-to-learn and
ambiguous data loaders respectively and then ap-
plies MixUp to a randomly sampled mini-batch of
each loader. We train a model on the generated TD-
MixUp samples in addition to the easy-to-learn and
ambiguous samples using the cross entropy-loss.
3 Experiments and Results
3.1 Tasks and Datasets
We evaluate our TDMixUp on three natural lan-
guage understanding tasks. We describe our in-
domain and out-of-domain sets as follows.
Natural Language Inference (NLI) Stanford
Natural Language Inference (SNLI) is a task to
predict if the relation between a hypothesis and
a premise is entailment, contradiction, orneutral
(Bowman et al., 2015). Multi-Genre Natural Lan-guage Inference (MNLI) captures NLI with diverse
domains (Williams et al., 2018).
Paraphrase Detection Quora Question Pairs
(QQP) is a paraphrase detection task to test if two
questions are semantically equivalent (Iyer et al.,
2017). TwitterPPDB (TPPDB) is a dataset built
to determine whether sentence pairs from Twitter
convey similar semantics when they share URLs
(Lan et al., 2017).
Commonsense Reasoning Situations With Ad-
versarial Generations (SWAG) is a commonsense
reasoning task to choose the most plausible contin-
uation of a sentence among four candidates (Zellers
et al., 2018). HellaSWAG is a dataset built using
adversarial filtering to generate challenging out-of-
domain samples.
3.2 Experimental Setup
We use BERT (Devlin et al., 2019) based classi-
fication model and pass the resulting [CLS] rep-
resentation through a fully connected layer and
softmax to predict the label distribution. We follow
the published train/validation/test datasets splits as
described in Desai and Durrett (2020). To iden-
tify mis-labeled samples in the top 33% easy-to-
learn samples, we set threshold values kas: the
80th/80th/50th percentile threshold sample AUMs
on SNLI/QQP/SWAG, respectively. More training
details and hyper-parameter settings can be found
in the Appendix. We evaluate the capability of
our TDMixUp strategy to improve both predictive
performance and model calibration due to its regu-
larization effect through data augmentation. Hence,
we use two metrics: (1) accuracy, and (2) expected
calibration error (ECE) (Guo et al., 2017; Desai
and Durrett, 2020). We report results averaged
across 5 fine-tuning runs with random restarts for
all experiments.
3.3 Baseline Methods
BERT (Devlin et al., 2019) is the pre-trained
base BERT model fine-tuned on each downstream
task.
Back Translation Data Augmentation (Edunov
et al., 2018) generates augmented samples by using
pre-trained translation modelswhich can generate
diverse paraphrases while preserving the semantics4246
of the original sentences. In experiments, we trans-
late original sentences from English to German and
then translate them back to English to obtain the
paraphrases.
MixUp (Zhang et al., 2018) generates augmented
samples by interpolating random training samples
in the input space (obtained from the first layer of
the BERT pre-trained language model).
Manifold MixUp (M-MixUp) (Verma et al.,
2019) generates additional samples by interpolat-
ing random training samples in the feature space
(obtained from the task-specific layer on top of the
BERT pre-trained language model).
MixUp for Calibration (Kong et al., 2020) gen-
erates augmented samples by utilizing the cosine
distance between samples in the feature space.
Note that the above baselines use 100% training
data while our proposed method focuses on partic-
ular subsets of the training data.
3.4 Results
Fine-tuning on Subsets of Training Data To
explore the effect of different subsets of the data
that are characterized using training dynamics, we
compare the result of BERT fine-tuned on 100%
training data with the results of BERT when fine-
tuned on these subsets, and show the comparison
in Table 1. Note that, for each task, we train the
model on in-domain training set, and evaluate onin-domain and out-of-domain test sets. We make
the following observations: First, we observe that
accuracy and ECE improve when we filter out pos-
sibly mis-labeled samples in the top 33% easy-to-
learn samples by using AUM in all cases (for both
in-domain and out-of-domain test sets). Specifi-
cally, using 24% train, Easy-to-learn with AUM
returns better accuracy and lower ECE than 33%
train, easy-to-learn , showing that there are some
potentially erroneous samples that harm the per-
formance in the top 33% easy-to-learn samples.
We manually investigate filtered samples on the
top 33% easy-to-learn samples and observe that
the top 33% easy-to-learn samples indeed include
mis-labeled samples. For example, in SNLI, we ob-
serve that the relation between the following pairs
of sentences in the top 33% easy-to-learn samples
iscontradiction when it should be neutral :<Two
opposing wrestlers competing to pin one another.;
‘Two women are shopping in a boutique. ’> and<‘A
person dressed in a colorful costume is holding
some papers. ’; ‘the cat jumps on the dog. ’> . In
contrast, we observe that in many cases accuracy
and ECE worsen when we filter out possibly mis-
labeled samples in the 33% ambiguous samples by
using AUM, suggesting that all ambiguous samples
are useful for learning and generalization (which
is consistent with Swayamdipta et al. (2020)). Sec-4247
ond, we observe that fine-tuning BERT on both
the easy-to-learn and the ambiguous samples ( 66%
train, Easy-to-learn & Ambiguous ) achieves simi-
lar performance and ECE as 100% train .
Main Results Table 2 shows the result of the
comparison of our proposed TDMixUp and base-
line methods. We observe that our proposed
method generally achieves higher accuracy and
lower ECE on both in-domain and out-of-domain
settings compared to any baseline using the full
100% training data showing the effectiveness of
our TDMixUp strategy.
3.5 Ablation Study
To compare the impact of the MixUp operation on
samples generated by random pairing and on sam-
ples generated by informative pairing , we conduct
an ablation study. Specifically, we compare the
results of MixUp on 66% train set (i.e., conduct the
MixUp operation between randomly selected sam-
ples on 66% train set, which is the union of the top
33% easy-to-learn and the top 33% ambiguous sam-
ples) and our proposed TDMixUp (i.e., conduct the
MixUp operation between the easy-to-learn filtered
by AUM and the ambiguous samples). As shown in
Table 3, we observe that our proposed TDMixUp
which selects informative samples to combine per-
forms better with respect to accuracy and ECE than
vanilla MixUp that selects random samples, in all
cases (in-domain and out-of-domain).
4 Conclusion
In this work, we propose a novel MixUp that lever-
ages training dynamics (confidence, variability, and
Area Under the Margin) to allow more informative
samples to be combined in generating augmented
samples. We empirically validate that our method
not only achieves competitive accuracy but also
calibrates BERT model on various NLP tasks, both
on in-domain and out-of-domain settings.
Acknowledgements
This research is supported in part by NSF CAREER
award #1802358 and NSF CRI award #1823292.
Any opinions, findings, and conclusions expressed
here are those of the authors and do not necessarily
reflect the views of NSF. We thank AWS for com-
puting resources. We also thank our anonymous
reviewers for their constructive feedback.4248References4249
A Supplementary Materials
A.1 Training Details
In our experiments, we use bert-base-uncased clas-
sification model on top of a task-specific fully-
connected layer. The model is fine-tuned with a
maximum of 3 epochs, batch size of 16 for SNLI
and QQP, batch size 4 for SWAG, a learning rate of
1e-5, gradient clip of 1.0, and no weight decay. We
use the hyper-parameter of MixUp αas 0.4. All
hyper-parameters are estimated on the validation
set of each task. For all results, we report aver-
aged results across 5 fine-tuning runs with random
starts. Finally, all experiments are conducted on
a single NVIDIA RTX A6000 48G GPU with the
total time for fine-tuning all models being under
24 hours. For each dataset, we follow the pub-
lished train/validation/test split by Desai and Dur-
rett (2020) and show the statistics of the datasets in
Table 4.
Dataset Train Dev Test
SNLI 549,368 4,922 4,923
MNLI 392,702 4,908 4,907
QQP 363,871 20,216 20,217
TwitterPPDB 46,667 5,060 5,060
SWAG 73,547 10,004 10,004
HellaSWAG 39,905 5,021 5,021
A.2 Data Maps
In this section, we provide data maps
(Swayamdipta et al., 2020) of our in-domain
datasets on bert-base-uncased model in Figure
1. These data maps are used to identify the
characteristics of each training sample (i.e.,
easy-to-learn, ambiguous, and hard-to-learn).4250