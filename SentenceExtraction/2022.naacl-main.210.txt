
Tom AyoolaJoseph FisherAndrea Pierleoni
Amazon Alexa AI
Cambridge, UK
{tayoola, fshjos, apierleo}@amazon.com
Abstract
Recent work in entity disambiguation (ED) has
typically neglected structured knowledge base
(KB) facts, and instead relied on a limited sub-
set of KB information, such as entity descrip-
tions or types. This limits the range of contexts
in which entities can be disambiguated. To
allow the use of all KB facts, as well as descrip-
tions and types, we introduce an ED model
which links entities by reasoning over a sym-
bolic knowledge base in a fully differentiable
fashion. Our model surpasses state-of-the-art
baselines on six well-established ED datasets
by 1.3 F1 on average. By allowing access to
all KB information, our model is less reliant
on popularity-based entity priors, and improves
performance on the challenging ShadowLink
dataset (which emphasises infrequent and am-
biguous entities) by 12.7 F1.
1 Introduction
Entity disambiguation (ED) is the task of linking
mentions of entities in text documents to their cor-
responding entities in a knowledge base (KB). Re-
cent ED models typically use a small subset of KB
information (such as entity types or descriptions)
to perform linking. These models have strong per-
formance on standard ED datasets, which consist
mostly of entities that appear frequently in the train-
ing data.
However, ED performance deteriorates for less
common entities, to the extent that many re-
cent models are outperformed by outdated fea-
ture engineering-based ED systems on datasets
that focus on challenging or rare entities (Prova-
torova et al., 2021). This suggests models over-rely
on prior probabilities, which are either implicitly
learned or provided as features, rather than make
effective use of the mention context. One reason
for this is that the subset of KB information used by
the models is not enough to discriminate betweensimilar entities in all contexts, meaning the model
has to fall back on predicting the most popular en-
tity. Another explanation for the performance drop
is that less common entities are prone to missing
or inconsistent KB information (e.g. they may not
have a description), which is problematic for mod-
els which rely on a single source of information.
To illustrate, we find that 21% of the 25% least
popularentities in Wikidata have neither an En-
glish description nor any entity type, leaving no
mechanism for models which rely on these two
sources of information alone to disambiguate them
(other than their label).Over half of these entities
have at least one KB fact (e.g. [Cafe Gratitude],
[headquarters location], [San Francisco]); so by
including KB facts the percentage of the least pop-
ular entities with no information aside from a label
drops from 21% to 8%.Figure 1: Example of a sentence where fine-grained KB
information is required for entity disambiguation.
In light of this, we introduce an ED model which
has access to entity types and descriptions, and
all KB facts. By using a larger variety of infor-
mation, our model is more robust to missing KB
information, and is able to disambiguate entities
in a broader range of contexts without relying on
entity priors. Figure 1 shows an example sentence
where there is insufficient information in the entity2899descriptions and types to disambiguate the men-
tion, Clinton . Fine-grained KB information, such
as facts about the birthplace or education of candi-
date entities, is required.
To incorporate KB facts, our model begins by re-
ranking candidate entities using descriptions (Wu
et al., 2019) and predicted entity types (Raiman
and Raiman, 2018). We then predict, using the doc-
ument context, the relations which exist between
every pair of mentions in the document. For exam-
ple, given the sentence in Figure 1, the model may
predict that the [place of birth] relation exists be-
tween the mention Clinton and the mention Hope,
Arkansas .For this, we introduce a novel “coarse-
to-fine” document-level relation extraction (RE)
module, which increases accuracy and reduces in-
ference time relative to the standard RE approach.
Given the relation predictions, we query the KB
(Wikidata in our case) for facts which exist between
any of the candidate entities for the mention Clin-
tonand for the mention Hope, Arkansas . In this
case we would find the Wikidata fact [Bill Clinton],
[place of birth], [Hope], and would correspond-
ingly boost the scores of both the [Bill Clinton] and
[Hope] entities. We implement this mechanism
with the KB stored in a one-hot encoded sparse
tensor, which makes the architecture end-to-end
differentiable.
Our model surpasses state-of-the-art (SOTA)
baselines on well-established ED datasets by 1.3
F1 on average, and significantly improves perfor-
mance on the challenging ShadowLink dataset by
12.7 F1. In addition, the model predictions are in-
terpretable, in that the facts used by the model to
make predictions are accessible.
Our contributions are summarised as follows:
1.We empirically show that using KB facts for
ED increases performance above SOTA meth-
ods, which generally rely on a single source
of KB information.
2.We introduce a scalable method of incorpo-
rating symbolic information into a neural net-
work ED model. To our knowledge, this is
the first time an end-to-end differentiable sym-
bolic KB has been used for ED.
3.We introduce a novel document-level rela-
tion extraction (RE) architecture which usescoarse-to-fine predictions to obtain competi-
tive accuracy with high efficiency.
2 Related Work
Recent work on ED has primarily focused on
feature-based approaches, whereby a neural net-
work is optimised so that the representation of the
correct KB entity is most similar to the mention
representation, and each mention is resolved in-
dependently. The way in which the KB entities
are represented varies between work. Initial work
(Ganea and Hofmann, 2017) learned entity embed-
dings directly from training examples, which per-
formed well for entities seen during training, but
could not resolve unseen entities. More recent work
improved performance on common datasets by en-
abling linking to entities unseen during training by
using a subset of KB information to represent enti-
ties, such as entity descriptions (Logeswaran et al.,
2019; Wu et al., 2020) or entity types (Raiman and
Raiman, 2018; Onoe and Durrett, 2020).
2.1 ED with KB context
Mulang’ et al. (2020) and Cetoli et al. (2019) in-
corporate KB facts into ED models by lexicalising
KB facts and appending them to the context sen-
tence, then using a cross-encoder model to predict
whether the facts are consistent with the sentence.
Our model differs from this approach as we resolve
entities in the document collectively rather than
independently; enabling pairwise dependencies be-
tween entity predictions to be captured. Another
potential limitation of the cross-encoder method
is the high computational cost of encoding the
long sequence length of every fact appended to
the document context. By accessing KB facts from
sparse tensors, we are able to avoid this bottleneck
and scale to a larger volume of facts (Cohen et al.,
2020).
2.2 ED with knowledge graph embeddings
Graph neural networks (GNN) have been used
to represent KB facts to inform ED predictions
(Sevgili et al., 2019; Ma et al., 2021). These ap-
proaches can potentially access the information in
all KB facts, but are reliant on the quality of the
graph embeddings, which may struggle to represent
many basic semantics (Jain et al., 2021) particularly
for unpopular entities (Mohamed et al., 2020).
2.3 Global ED
There has been a series of papers which aim to
optimise the global coherence of entity choices2900
across the document (Hoffart et al., 2011; Cheng
and Roth, 2013; Moro et al., 2014; Pershina et al.,
2015). Our model differs from previous approaches
in that the model predicts the relations which exist
between mentions based on the document text and
weights the coherence scores by these predictions,
rather than considering coherence independently
of document context. We also limit the model to
pairwise coherence between mentions as opposed
to global coherence for computational efficiency.
2.4 ED with multiple modules
The most similar work to ours is Orr et al. (2021),
which achieves strong results on tail entities by
introducing an ED model which uses entity embed-
dings, relation embeddings, type embeddings, and
a KB module to link entities. A key difference to
our model is the way in which KB facts are used
for disambiguation. In their work, KB facts are
encoded independently of the document context in
which the candidate entities co-occur, whereas our
model is able to leverage the relevant KB facts for
the document context.
3 Proposed Method
3.1 Task formulation
Given a document Xwith mentions, M=
{m, m, ...m}, a KB with a set of facts G=
{(s, r, o )⊂E×R×E}which express relations
r∈Rbetween subject s∈Eand object entities
o∈E, and a description dfor each KB entity e,the goal of ED is to assign each mention m∈M
the correct corresponding KB entity e∈E.
3.2 Overview
Figure 2 shows a high-level overview of our model.
We use a transformer model to encode all mentions
in the document in a single-pass. We use these
mention embeddings both to generate initial candi-
date entity scores for each mention using the entity
types and descriptions of KB entities and to pre-
dict relations between every pair of mentions in
the document. We retrieve KB facts for every pair
of mentions in the document, for each combina-
tion of candidate entities. We weight the retrieved
KB facts by multiplying the initial candidate entity
score for the subject entity, the predicted score for
the relation, and the initial candidate entity score
for the object entity. Then we generate KB scores
by summing the weighted facts for each candidate
entity. The final score used for ranking entities is a
weighted sum of the initial score and KB score.
3.3 Mention representation
We encode the tokens in the document Xusing a
transformer-based model, giving contextual token
embeddings H={h,h, ...,h}.We obtain
mention embeddings mfor each mention mby
average pooling the contextualised token embed-
dings of the mention from the final transformer
layer. This allows all mentions Min the document2901Xto be encoded in a single forward pass.
3.4 Initial entity score ψ
Initially, we score candidate entities using entity
typing and description scores. We combine the two
with a learned weighted sum ψ:
ψ(c) =wψ(c) +wψ(c) (1)
where cis the mention-entity pair ( m,e),ψis
a scoring function based on candidate entity types,
andψis a scoring function based on candidate
entity descriptions.
3.4.1 Entity typing score ψ
We construct a fixed set of types T={(r, o)⊂
R×E}by taking relation-object pairs (r, o) from
the KB G; for example (instance of, song). We pre-
dict an independent unnormalised score for each
typet∈Tfor every mention in the document by
applying a linear layer FFto the mention em-
bedding m. To compute entity scores ψusing
the predicted types, we calculate the dot product
between the predicted types and the candidate en-
tity’s types binary vector t.Additionally, we
add a P(e|m)(PEM score) which expresses the
probability of an entity given the mention text only,
and is obtained from hyperlink count statistics as
in previous work (Raiman and Raiman, 2018):
ψ(c) = ( FF(m)·t) +P(e|m) (2)
3.4.2 Entity description score ψ
We use a bi-encoder architecture similar to (Wu
et al., 2019) but altered to encode all mentions in
a sequence in a single forward pass, as opposed
to requiring one forward pass per mention. We
represent KB entities as:
[CLS] label [SEP] description [SEP]
where “label” and “description” are the tokens of
the entity label and entity description in the KB.
We refer to this as d. To compute entity scores ψ
using entity descriptions, we use a separate trans-
former model TRto encode d, taking the final
layer embedding for the [CLS], and calculate the
dot product between this embedding and the con-
textual mention embedding mprojected by linear
layer FF:
ψ(c) =FF(m)·TR(d) (3)3.5 Relation extraction
Our relation extraction layer outputs a relation
score vector ˆ r∈Rfor each pair of mentions
mandmin the document, where Ris the subset
of relations chosen from the KB. To calculate ˆ r
we begin by passing mandmthrough a bilin-
ear layer Bwith output dimension 1, to predict
the likelihood ˆr that a relation exists between
mentions mandm.
ˆr =σ(B(m,m)) (4)
Note that ˆr is a scalar, denoting the likeli-
hood that anyrelation exists between mention m
andm.
We then take the top-k mention pairs with the
highest values of ˆr (in similar style to Lee
et al. (2018) who introduce a coarse-to-fine ap-
proach for coreference resolution), illustrated with
K= 2in Figure 3. These are the pairs of mentions
which the model predicts have the highest likeli-
hood of having a relation connecting them. For
the surviving mention pairs, we pass each of the
two mention embeddings individually through a
linear layer, FF, to reduce their dimension by a
factor of two. This ensures that when we concate-
nate the two representations back together we get a2902representation of the mention pair mof the same
dimension as the contextual token embeddings H.
m=concat (FF(m),FF(m)) (5)
We then pass the resulting embedding m
through a series of transformer layers TR, where
they can attend to the contextual embeddings of
the original input tokens, H={h,h, ...,h}.
The mention-pair embeddings from the final trans-
former layer are passed through a linear layer
FFwith output dimension |R|to give the score
that each relation exists between this mention-pair,
ˆ r.
ˆ r=FF(TR(m, H))) (6)
Finally, to get ˆ rwe multiply the coarse layer
score ˆr with the fine layer score ˆ r, en-
suring that gradients are propagated through the
coarse layer during training, despite only the top-k
mention pairs being passed to the fine layer.
ˆ r= ˆr∗ˆ r (7)
For all mention pairs outside the top-k pairs, we
setˆ rto a vector of 0s.
The relation extraction layer is trained end-to-
end using the signal from the entity disambigua-
tion loss only, and is not pretrained with any task-
specific relation extraction data. To validate the
effectiveness of the architecture, we include results
with the RE module trained in isolation on the DO-
CRED RE dataset in Appendix D.
3.6 KB score ψ
We retrieve KB factsfor every mention-entity pair
in the document and represent it as a 5-dimensional
tensor r, where ris a binary vector indicat-
ing the relations that exist in the KB between the
two entities ( eande) for mention-entity pair c
andc.We weight KB facts rbased on initial
entity scores ψand relation predictions ˆ r, accord-
ing their relevance to the document. To compute
the KB score ψfor a mention-entity pair, we sum
KB facts where the entity (from the mention-entity
pair) is the subject entity to give score ψand sum
the KB facts where the entity is the object entity to
give score ψ:
where ˙ψis the initial entity scoring function
ψfollowed by the softmax function applied over
the candidate entities for the given mention. We
then combine the two scores with a weighted sum
giving ψ:
ψ(c) =wψ(c) +wψ(c) (10)
Note that for computational efficiency, this scor-
ing mechanism considers the coherence of entity
predictions between pairs of mentions only, in con-
trast to methods which consider global coherence
Hoffart et al. (2011).
3.7 Optimisation and inference
To obtain final entity scores ψ, we add the KB
scores ψto the initial entity scores ψ.
ψ(c) =ψ(c) +ψ(c) (11)
We train our model on entity linked documents
using cross-entropy loss. Our model is fully differ-
entiable end-to-end, with the training signal propa-
gating through all modules, including the relation
extraction module. During ED inference, we take
the candidate entity with the highest final entity
score for each mention.
4 Experiments
4.1 Standard ED
We evaluate our model on the following well-
established standard ED datasets: AIDA-CoNLL
(Hoffart et al., 2011), MSNBC (Cucerzan, 2007),
AQUAINT (Milne and Witten, 2008), ACE2004
(Ratinov et al., 2011), CWEB (Gabrilovich et al.,
2013) and WIKI (Guo and Barbosa, 2018). We
train our model on Wikipedia hyperlinks and report
InKB micro-F1 (which only considers entities with
a non-NIL entity label). To ensure fair comparisons
to baselines, we use the same method to generate
candidates as previous work (Cao et al., 2021; Le
and Titov, 2018). Concretely, we use the top-30
entities based on entity priors (PEM) obtained by
mixing hyperlink count statistics from Wikipedia
hyperlinks, a large Web corpus, and YAGO.29034.2 Long-tail and ambiguous ED
We use the ShadowLink ED dataset (Provatorova
et al., 2021) to evaluate our model on long-tail and
ambiguous examples.The dataset consists of 3
subsets. SHADOW where the correct entity is over-
shadowed by a more popular entity; TOP where the
correct entity is the most popular entity; and TAIL
where the correct entity a long-tail entity.All
examples in SHADOW and TOP are ambiguous,
whereas TAIL has some unambiguous examples,
as it is a representative sample of long-tail entities.
The original dataset consists of short text snippets
from Web pages, which often only include one or
two mentions of entities. This limits the ability of
our model to use its document-level RE module,
and reason over the relationships between entities.
We therefore also evaluate on the full-text version
of the SHADOW and TOP subsets, referred to as
SHADOW-DOC and TOP-DOC in the results ta-
bles.The dataset consists of 1 annotated entity
per document, so we run spaCy (“en_core_web_lg”
model) (Honnibal and Montani, 2017) to identify
additional mentions to allow our model and base-
lines to utilise other mentions to disambiguate the
annotated entity mention.
4.3 Model details
We use Wikidata (July 2021) as our KB, restricted
to entities with a corresponding English Wikipedia
page. This results in 6.2M entities. We use this
data to generate lookups for entity types, entity
descriptions, and KB facts. We select a fixed set
of 1400 relation-object pairs, based on usefulness
for disambiguation, to use as our entity types (Ap-
pendix A). For the KB facts, we represent the top
128 relations as separate classes and collapse the
remaining relations into a single class we refer to
asOTHER . Additionally, we add a special relation
which exists between every entity and itself. We re-
fer to this relation as the SAME AS relation, and the
idea behind this is to enable the model to implicitly
learn coreference resolution.
4.4 Training details
We use Wikipedia hyperlinks (July 2021) with ad-
ditional weak labels as our training dataset, whichconsists of approximately 100M labelled mentions.
We limit candidate generation to top-30 entities
based on entity priors obtained from Wikipedia
hyperlink statistics.Our model operates at the
document-level and is trained using multiple men-
tions simultaneously. We initialise the mention
embedding Transformer model weights from the
RoBERTa (Liu et al., 2019) model and train our
model for 1M steps with a batch size of 64 and
a maximum sequence length of 512 tokens. This
requires approximately 4 days when using 8 V100
GPUs. For additional details, see Appendix B.
5 Results
5.1 Standard ED
The results in Table 1 show our model (KBED)
achieves the highest average performance across
the datasets by a margin of 1.3 F1, reducing er-
rors by 11.5%. The ablation results indicate the
majority of the improvements across the datasets
are attributable to our novel KB module. We ob-
serve the largest improvement of 3.0 F1 on the
WIKI dataset, which is likely due to the documents
having high factual density, enabling our model to
leverage more KB facts (see Section 6.1 for rela-
tion analysis). Despite our model only be trained
on Wikipedia, we obtain competitive results on out-
of-domain datasets, such as MSNBC news articles,
which implies the patterns learned from Wikipedia
are applicable to other domains. In addition, the
results demonstrate that our 3 modules (entity typ-
ing, entity descriptions, and KB facts) are comple-
mentary; when any module is used in isolation it
reduces performance, demonstrating the benefits of
a multifaceted approach to ED. Surprisingly, when
our KB module is used in isolation it performs
on par with the TagMe baseline, which suggests
there is reasonable overlap between KB facts and
the facts predicted from documents. Note that the
AIDA results in Table 1 contain a mixture of mod-
els fine-tuned on this dataset (denoted with **) and
trained on Wikipedia only (as in our case), so the
numbers are not directly comparable.
5.2 Long-tail and ambiguous ED
Our model achieves an average F1 score of 70.1 on
the original ShadowLink dataset (Table 2) which
substantially outperforms (+16.5 F1) embeddings-
based models (GENRE, REL) and moderately out-
performs (+4.0 F1) the Bootleg model (Orr et al.,2904
2021) which is optimised for tail-performance and
also uses entity types and KB facts. On the original
dataset, the impact of our KB module is negligi-
ble because the limited document context reduces
the chances of KB-related entities co-occurring;
the strong performance is therefore largely due to
the combination of entity types and descriptions.
However, we see a notable average improvement
of 12.7 F1 on the document-level version of the
dataset, with the KB module having a consider-
able impact especially on the overshadowed entity
subset where it contributes 6.7 F1. The perfor-
mance margin between our model and Bootleg is
greater when document-level context is used likely
because Bootleg is designed for short contexts and
has limited control over which KB facts to use for
disambiguation, as all facts are weighted uniformly.
We include a more extensive model ablation study
in Appendix C.
5.3 Relation extraction module
To analyse the impact of the doc-level RE architec-
ture introduced in Section 3.5 we present results
in Tables 1 and 2 of performance with a standard
bilinear RE layer (Xu et al., 2021). Our RE archi-
tecture leads to an average increase of 0.9 F1 on
the standard ED datasets, of 1.3 F1 on the stan-
dard ShadowLink splits, and of 1.5 F1 on the Shad-
owLink doc-level splits. In addition, by avoiding
the quadratic complexity bilinear layer, we achieve
an increase in inference speed of approximately2x, as measured on AIDA documents. We include
doc-level RE results for our architecture on the
DOCRED (Yao et al., 2019) dataset in Appendix
D.
5.4 Error Analysis
In Table 3 we show the results from annotating
50 examples in which the model made an incor-
rect prediction for both the AIDA test split and the
ShadowLink SHADOW-DOC split. Gold not in
cands. refers to cases in which the gold entity was
not in the top-30 candidates from the PEM table;
Missing KB fact are cases where the model cor-
rectly predicted a relation connecting two mentions,
but the corresponding fact was not in the KB; Dom-
inant PEM is when the initial PEM score for one
candidate was high (> 0.8), and the model fails to
override this score; Incorrect RE pred. are cases
in which the model makes an incorrect RE predic-
tion between two mentions, and where this wrong
prediction leads to the wrong choice of entity; Am-
biguous ann. refers to gold annotations that are
either incorrect or ambiguous.
The results in Table 3 indicate that the largest
source of error is the gold entity not being present
in the top-30 candidates. This is particularly true
for the ShadowLink SHADOW-DOC split, as this
split contains a larger number of tail entities which2905
are less likely to be mentioned on Wikipedia. For
the AIDA dataset, there are also many cases which
are in some sense ambiguous.There are 8 cases
in total where the model predicts a relation which
it expects to be in the KB, but which is not in fact
present. This is largely in the ShadowLink split,
where tail entities are likely to be less well repre-
sented in Wikidata. The model is generally good at
not depending on entity priors; despite every gold
candidate in the Shadowlink SHADOW-DOC split
being “overshadowed” by a more popular entity in
the PEM table, there is only one example where the
model fails to override this. Although the model of-
ten “over-predicts” relations between mentions, it
rarely gets penalised for doing so, as in general the
extra facts it predicts are not in the KB, meaning
theIncorrect RE pred. count is low.
To further explore the role of missing candidates,
Table 4 shows the percentage of the gold entities
present in the top-30 candidates we pass to the
model, representing a hard upper-bound on the re-call our model can achieve. The results vary from
a high coverage of 99.5 for the MSNBC dataset,
which largely contains head entities, to a lower cov-
erage for the ShadowLink SHADOW (75.3) and
TOP (83.6) splits. Table 4 also shows the coverage
if we pass all PEM candidates to the model. For
some datasets, such as WIKI, this increases the cov-
erage significantly. However, for the ShadowLink
SHADOW split, the coverage is still below 80%, in-
dicating that better candidate generation strategies
are an interesting avenue for future research.
6 Analysis
6.1 Relation predictions
To understand the relations which the model utilises
to make predictions, Table 5 displays for the WIKI
dataset the number of KB (Wikidata) facts which
exist between gold annotated mentions in the doc-
uments ( Gold ), the number of facts between men-
tions our model predicts with a score above 0.5
(Predicted ) and the percentage of gold facts which
our model also predicts ( Recall ).2906TheSAME AS relation is used extensively by the
model, demonstrating that using coreferences to
other (potentially easier to disambiguate) mentions
of the same entity in the document is a powerful ad-
dition for ED. We leave evaluation of the model on
the coreference-specific task to future work. The
OTHER relation is also commonly predicted, sug-
gesting the long tail of relations in Wikidata still
hold useful information. The other widely used
relations are generally either geographical or sports
related, which is expected given the large number
of sports entities in Wikidata.
The recall numbers appear low, although this is
expected behaviour in that the existence of a Gold
fact does not necessarily imply that the text in the
document infers this fact. For example, the text
“Donald Trump visited New York” would include
the gold fact [Donald Trump] [place of birth] [New
York] but making this prediction for all sentences
of this form would likely harm performance.
7 Conclusion
We presented a novel ED model, which achieves
SOTA performance on well-established ED
datasets by a margin of 1.3 F1 on average, and by
12.7 F1 on the challenging ShadowLink dataset.
These results were achieved by introducing a
method to incorporate large symbolic KB data into
an ED model in a fully differentiable and scalable
fashion. Our analysis shows that better candidate-
generation strategies are an interesting avenue for
future research, if results are to be pushed higher
on ambiguous and tail entities. Dynamic expansionof the KB by incorporating facts identified by the
ED model is also a potentially promising direction.
Acknowledgements
We would like to thank Vera Provatorva for provid-
ing us with the extended version of the ShadowLink
dataset and Laurel Orr for assisting us with running
the Bootleg baseline.2907References29082909A Entity Type Selection
Our entity types are formed from direct Wikidata
relation-object pairs and relation-object pairs in-
ferred from the Wikidata subclass hierarchy; for
example, (instance of, geographical area) can be
inferred from (instance of, city). We only consider
types with the following relations: instance of, oc-
cupation, country, and sport. We select types by
iteratively adding types that separate (assuming an
oracle type classifier) the gold entity from negative
candidates for the most examples in our Wikipedia
training dataset.
B Training Details
We use the Hugging Face implementation of
RoBERTa (Wolf et al., 2019) and optimise our
model using Adam (Kingma and Ba, 2015) with a
linear learning rate schedule. We ignore the loss
from mentions where the gold entity is not in the
candidate set. Our model has approximately 197M
trainable parameters. We present our main hyperpa-
rameters in Table 6. Due to the high computational
cost of training the model, we did not conduct an
extensive hyperparameter search. To reduce GPU
memory usage to below 16 GB during training, we
subsample 30 mentions per context window, and
subsample 5 candidates per mention (subsampling
is not required during inference).
C Model Ablation Study
In this section, we measure the contribution of key
aspects of our model. For each model ablation, wetrain our model from scratch on the AIDA-CoNLL
training set and evaluate on the development set,
keeping hyperparameters constant. Surprisingly,
the performance of our model is strong in this lim-
ited data setting, which means that our model is not
dependent on a large set of training examples when
there is a small amount of annotated in-domain
data. Note that for “w/o 128 standard relations”
we collapse all standard relations into the OTHER
special relation; and for “w/o RE transformer” we
replaced the RE transformer with a single bilinear
layer. Our results (Table 7) indicate that all aspects
of our model that we measured have a positive im-
pact on performance. Interestingly, the KB module
(+2.2 F1) has a greater impact than the entity de-
scription (+0.48 F1) and entity typing (+0.9 F1)
modules despite weaker performance when used
on its own (Table 1). This implies there is less over-
lap between examples where KB module performs
well, and the other modules perform well. We ob-
serve, the SAME AS relation improves performance
by 0.72 F1, which demonstrates that using corefer-
ence improves ED. Finally, we find that when the
KB module has greater control over how to weight
KB facts (based on the context) it leads to better
results, for example if we collapse all standard re-
lations into a single relation our performance drops
by 0.7 F1.2910D Doc-level RE results on DOCRED
To verify the performance of our document-level
RE architecture introduced in Section 3.5, we
present results of models trained and evaluated
on the DOCRED dataset (Yao et al., 2019). Our
baseline implementation uses roberta-base as an
encoder and a bilinear output layer. We show two
variants in Table 8, a bilinear layer with input di-
mension 128 and with input dimension 256, which
give an F1 score of 57.8 and 58.4 respectively. This
compares to a score of 59.5 for an equivalent base-
line implemented in (Xu et al., 2021). The differ-
ence is explained by our baseline not giving the
model access to the gold coreference information,
which is allowed in the DOCRED task but which
we exclude as it will not be available for our entity
linking task.
Our coarse-to-fine approach, with 4 “fine” trans-
former layers, pushes the dev-level F1 up by 2.8 F1
to 61.2. This puts it slightly above the roberta-base
version of the current state-of-the-art model, SSAN
(Xu et al., 2021), which scores 60.9, and addition-
ally has access to the gold coreference labels in the
embedding layer of the model. This validates that
our document-level RE architecture is capable of
producing accurate relation predictions, which we
see in the main results table (Table 1) also translates
into stronger ED performance.
By avoiding the bilinear layer, our implementa-
tion is also faster to train, achieving 106.2 seconds
per epoch on the DOCRED dataset on a single
Tesla V100 GPU, compared to 155.7 seconds for
the baseline model with a 128-dimension bilinear
layer, and 343.2 seconds for the more accurate base-
line model with a 256 dimension bilinear layer.
E Dataset details
E.1 Dataset statistics
We present the topic, number of documents and
number of mentions for each dataset used for eval-
uation (Table 9). The datasets used cover a varietyof sources including wikipedia text, news articles,
web text and tweets. Note that the performance of
the model outside these domains may be signifi-
cantly different.
E.2 ShadowLink Full Text versions
The authors of Provatorova et al. (2021) kindly
provided us with the full documents from which
the shorter text snippets (usually one or two sen-
tences) in the ShadowLink dataset were sourced.
We were able to match 596 of the 904 examples
in the SHADOW split to its corresponding docu-
ment, and 530 out of the 904 examples in the TOP
split. As some full articles were extremely long
we limited the document-length to 10000 charac-
ters, centred around the single annotated entity. To
validate that the subset of examples we were able
to match to full documents were representative of
the original dataset splits, we ran our model on the
sentence-level versions of these subsets, achieving
47.7 on the SHADOW split (comparable to 47.6 in
Table 2) and 63.9 on the TOP split (comparable to
64.2 in Table 2).
F Additional relation analysis
To expand on the analysis in Section 6.1 we also
include the number of gold and predicted rela-
tions in documents in the AIDA dataset (Table
10). The first clear difference is that there is a far
higher count of gold SAME AS facts in the AIDA
dataset, which is potentially explained by pages on
Wikipedia generally having hyperlinks for the first
mention of an entity only.
It is also interesting to note that there are lower
recall numbers for the AIDA dataset relative to
WIKI (Table 5), indicating that the RE module may
have “overfit” in some sense to the Wikipedia style
of article, and may be less effective on AIDA style
news articles.2911
G Inference Speed and Scalability
We measure the time taken to run inference on the
AIDA-CoNLL test dataset and compare it to SOTA
baselines. Table 11 shows the results alongside
the average ED performance on the 6 standard ED
datasets (used in Table 1). Our model is an order
of magnitude faster than the baselines with compa-
rable ED performance.
The most computationally expensive part of our
model (accounting for approximately 80% of the
inference and training time) is computing the KB
score due to the large number of pairwise interac-
tions present in documents. The hyperparameter
for coarse-to-fine relation extraction can be lowered
to trade-off computation cost with ED performance
by reducing the number of pairwise interactions.
Alternatively, as computation of the initial entity
score ψis computationally cheap relative to the
KB score ψ, candidate entities with low initial en-
tity scores can be pruned to further increase training
and/or inference speed. These approaches would
also allow scaling of the initial number of candi-
date entities to more than the 30 used for inference
in this paper, if the use case required it.2912