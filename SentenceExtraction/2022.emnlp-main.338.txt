
Hui Liu, Weidong Guo, Yige ChenXiangyang LiPlatform and Content Group, TencentCollege of Computer Science and Artificial Intelligence, Wenzhou University{pvopliu,weidongguo,xiangyangli}@tencent.comyigechen@wzu.edu.cn
Abstract
Headline generation is a task of generating
an appropriate headline for a given article,
which can be further used for machine-aided
writing or enhancing the click-through ra-
tio. Current works only use the article it-
self in the generation, but have not taken the
writing style of headlines into consideration.
In this paper, we propose a novel Seq2Seq
model called CLH3G ( Contrastive Learning
enhanced Historical Headlines based Headline
Generation) which can use the historical head-
lines of the articles that the author wrote in the
past to improve the headline generation of cur-
rent articles. By taking historical headlines into
account, we can integrate the stylistic features
of the author into our model, and generate a
headline not only appropriate for the article,
but also consistent with the author’s style. In
order to efficiently learn the stylistic features of
the author, we further introduce a contrastive
learning based auxiliary task for the encoder of
our model. Besides, we propose two methods
to use the learned stylisic features to guide both
the pointer and the decoder during the genera-
tion. Experimental results show that historical
headlines of the same user can improve the
headline generation significantly, and both the
contrastive learning module and the two style
features fusion methods can further boost the
performance.
1 Introduction
Natural Language Generation tasks have achieved
great success both in research and application, such
as Neural Machine Translation (Bahdanau et al.,
2014), Headline Generation (Jin et al., 2020; Ao
et al., 2021) and so on. In many real-life reading
scenarios, an attractive headline of the article can
immediately grab the readers and then lead them
to view the whole article. Thus, headline genera-
tion (HG) is becoming an important task and drawFigure 1: Comparison of general Headline Generation
and Historical Headlines based Headline Generation.
increasing attention nowadays, which aims to au-
tomatically generate the appropriate headline for a
given aritcle.
Earlier research of HG (Dorr et al., 2003; Tan
et al., 2017) mainly focus on generating a fluent
and relevant headline for a given news article to
alleviate the author’s work in a machine-aided writ-
ing way. More recent works (Zhang et al., 2018;
Xu et al., 2019; Jin et al., 2020) intend to generate
attractive headlines for articles so as to get higher
click-through ratio and further directly improve the
profit of the online social media platforms. Fur-
thermore, some works (Liu et al., 2020; Ao et al.,
2021) try to generate keyphrase-aware and person-
alized headlines to meet the requirements of differ-
ent application scenarios and further satisfy users’
personal interests.
However, most of these existing methods only
use the information of the article to generate the
headline but ignore the author’s historical head-
lines. In general, the headlines manually designed
by the author are usually more suitable for the au-
thor’s articles but the design style of the headline
may be quite different from the writing style of the
article and we can hardly obtain the author’s head-
line style only through the content of the article.
Therefore, when we integrate existing historical
headlines into the HG model to learn the headline
style of the author, such as grammar and syntax,
the model can generate more appropriate headlines
for machine-aided writing. For example, as shown
in Figure 1, all of the historical headlines and the5063reference headline are composed of two clauses.
The generated headline of the historical headlines
based HG model has the same syntax with the his-
torical headlines, which makes it more likely to be
accepted by the author and more attractive than the
generated headline with distinctive syntax by the
general HG model.
To our best knowledge, there is no corpus that
contains both news articles and corresponding au-
thorships to meet the requirement of our experi-
ments. Therefore, in this paper, we build a new
dataset named H3G( Historical Headlines Based
Headline Generation) to explore the research of
historical headlines based HG. We collect the H3G
dataset from the online social media platform Ten-
cent QQBrowser, which contains more than 380K
news articles from more than 23K different authors.
The detailed introduction of the H3G dataset is
discussed in the Experiment section.
Besides, we propose a novel Contrastive Learn-
ing enhanced Historical Headlines based Headline
Generation (CLH3G) model to extract and learn
headline styles for HG. Inspired by the existing
style transfer models (Lample et al., 2018; Dai
et al., 2019), we represent the headline style of
the historical headlines as a single vector. Such a
design can not only reduce the computation cost
of the historical headlines representation, but also
facilitate the integration of historical headlines in-
formation on the decoder side of the HG model.
Besides, two different methods are applied to guide
the generation of the author-style headlines through
the single headline vector. The first style vector fu-
sion method can instruct the decoder to generate
author-style target headline representation, and the
other controls the generated words of the pointer-
generator network. What’s more, on the encoder
side of Sequence-to-Sequence (Seq2Seq) model,
we also use Contrastive Learning (CL) to distin-
guish headlines from different authors as an auxil-
iary task, which is consistent with and conduce to
the extraction of the headline style.
Experimental results on automatic metrics
ROUGE and BLEU and Human evaluation show
that the historical headlines can greatly improve
the effectiveness of HG compared with general
HG models, and both of the two style vector fu-
sion methods and Contrastive Learning based aux-
iliary task can also improve the performance. We
also train a Contrastive learning classifier to dis-
tinguish headlines from different authors, and findour CLH3G model can generate more author-style
headlines than the general HG models and other
compared models.
To this end, our main contributions are summa-
rized as follows:
•We propose a new HG paradigm namely
Historical Headlines based HG to generate
author-style headlines, which can be used for
machine-aided writing and click-through ratio
enhancing.
•We propose a novel model CLH3G, which uti-
lizes two headline style vector fusion methods
and contrastive learning to make full use of
historical headlines.
•We construct a new Historical Headlines
based HG dataset namely H3G and conducted
abundant experiments on it. Experimental re-
sults show that the historical headlines are
beneficial to headline generation, and both the
two headline style vector fusion methods and
Contrastive Learning can also improve the HG
models.
2 Related Work
Headline Generation focus on generating a suit-
able or attractive headline for a given article. We
divide HG into three categories, namely general
HG, style-based HG and adaptive HG.
The general HG models want to generate a flu-
ent and suitable headline given an article. An
early work (Dorr et al., 2003) uses linguistically-
motivated heuristics to generate a matching head-
line. This method is very safe, because all words
in the generated headline are selected from the
original article. Then, some works (See et al.,
2017; Gavrilov et al., 2019) use End-to-End neural
networks to generate headlines. These methods
achieve the state-of-the-art results and are very con-
venient for training and inference. Besides, (Tan
et al., 2017) uses a coarse-to-fine model to generate
headlines for long articles.
The style-based headline generation models
aims to generate headline with specific styles. (Xu
et al., 2019) uses Reinforcement Learning to gen-
erate sensational headlines to capture reader’s in-
terest. (Zhang et al., 2018) proposes dual-attention
sequence-to-sequence model to generate question-
style headlines, because they find question-style5064
headlines can get much higher click-through ratio.
Besides, (Jin et al., 2020) uses parameter sharing
scheme to generate general, humorous, romantic,
click-baity headlines at the same time.
Adaptive headline generation models want to
generate different headlines for different scenarios.
(Liu et al., 2020) proposes to generate different
headlines with different keywords, which can be
used to generate different headlines for different
search queries in search engines. (Ao et al., 2021)
uses the user impression logs of news to generate
personalized headlines for different users to satisfy
their different interests.
Contrastive Learning is very popular recently
for representation learning. CL was first used for
vision understanding in (Chen et al., 2020). Sub-
sequently, CL is also used in Natural Language
Generation, including Conditional Text Generation
(Lee et al., 2020), Dialogue Generation (Cai et al.,
2020), Report Generation (Yan et al., 2021) and
text summarization (Liu and Liu, 2021). In this
paper, we use CL like (Chen et al., 2020), whose
framework includes a neural network encoder and
a small neural network projection.
3 Model
Figure 2 shows our proposed Contrastive Learn-
ing enhanced H3G (CLH3G) model, which is an
End-to-End Seq2Seq generation model. We will
briefly introduce the entire model in Section 3.1
and discuss the encoder and the CL based auxiliary
task in Section 3.2. Finally, the decoder and two
headline style vector fusion methods are presented
in Section 3.3.3.1 Problem and Architecture
Given an article and kheadlines from other articles
written by the same author, our model will gener-
ate a headline which is most suitable for this arti-
cle and consistent with the headline writing style
of the author. Formally, the CLH3G model uses
the article A= [w, w, ..., w]of the author X
and some historical headlines T= [t, t, ..., t]
ofXto automatically generate a new headline
H= [w, w, ..., w], which is suitable for A
and consistent with the headline writing style of
X.
Compared with previous HG methods, our
model put more emphasis on learning the style
of the input historical headlines to improve the per-
formance. Specifically, during encoding, we use a
single vector like (Lample et al., 2018; Dai et al.,
2019) to derive the style information from the input
headlines, and adopt CL to further distinguish the
style among different authors. The CL module will
not bring overhead because it shares the same en-
coder with the original HG model. Besides, we fuse
two different methods to integrate the style infor-
mation into the decoder: the first one is designed to
influence the representation of the generated head-
line, and the other will guide the pointer module
to copy author-style headline words. In the rest of
this section, we will introduce the CLH3G model
in detail.
3.2 Encoder and Contrastive Learning based
Auxiliary Task
Transformer Seq2Seq Model (Vaswani et al., 2017)
has achieved remarkable success in Natural Lan-
guage Generation. Transformer consists of a self-
attention multi-head encoder and a self-attention5065multi-head decoder. In order to enhance the se-
mantic representation capability of the encoder, we
use the pre-trained BERT-base model (Devlin et al.,
2018) to initialize the parameters of the encoder,
which can generate superior article representation
and headline vectors to improve the effectiveness
of HG models.
As shown in Figure 2, the encoder represents the
article AasH∈R, where dis the hidden
size of BERT-base. For each headline tinT, we
use the encoder outputs at [CLS]as its headline
representation, so all historical headlines Tare
represented as H∈R. Subsequently, we
average the historical representation Hto obtain
a single style vector s∈R, and His also
used to compute the CL loss.
Contrastive learning is a self-supervised method
that can learn knowledge from unlabeled data. Re-
cently, CL has achieved great success in many
fields (Chen et al., 2020; Liu and Liu, 2021). Same
as (Chen et al., 2020), our CL module consists of a
neural network base encoder and a small neural net-
work projection. The CL encoder and the CLH3G
encoder share parameters, so that we only need to
compute the headlines representation once for both
headline style vector and CL loss function to avoid
additional overhead. The projection is a two-layer
fully connected feed-forward network. Instead of
explicitly constructing positive examples like most
CL models, we regard the headline pairs belong-
ing to the same author as positive samples, and
the other headlines in the same batch belonging to
negative samples. The loss function of the positive
pair of examples ( i,j) is defined as
L=−logexp(sim(z, z)/τ)/summationtextIexp(z, z)/τ(1)
where Iis an indicator function evaluated to 1
iffk̸=iandτis a temperature parameter.
In the H3G dataset, all train and test samples
contain at least one historical headline. Certainly,
the CLH3G model can generate the general head-
line only with article information, but we mainly
want to explore the performance of the model when
adding historical headlines. During the training
phase, the target headline is also used in CL mod-
ule but not in the computation for the single style
vector. We randomly select two headlines from the
input historical headlines and target headline for
CL loss function. During the inference phase, we
do not use the CL module and the target headline,so that the CL module will not affect the inference
speed.
3.3 Decoder and Two Headline Style Vector
Fusion Methods
As shown in Figure 2, the shifted right target head-
lineHis imputed into the decoder to generate the
target headline representation matrix D∈R.
During the computation of decoder, our first head-
line style vector fusion method is simply concate-
nating the article representation Hand the single
style vector stoH∈R. This concate-
nated result Hcan guide the decoder to repre-
sent the shifted right target headline to generate a
new headline with the same style of headlines in
T. There are many overlapping words in the head-
line and the corresponding article, so we use the
pointer module same as (See et al., 2017) to solve
the out-of-vocabulary (OOV) problem and improve
the performance of generation models. Our second
headline style vector fusion method is to add the
single style vector to the pointer module. On the
one hand, we use the style vector to select words
of the input article for i−thgenerated word as:
α(i) =softmax (w[H:S]+
w[d:s] +b)(2)
where S∈Ris the result of srepeating a
times, and w,wandb are learn-
able parameters. We use the headline vector dto
produce the vocabulary distribution of the i−th
generated word as:
P(i) =softmax (Vd+b)(3)
where Vandb are learnable parameters.
On the other hand, the generation probability
P(i)∈[0,1]is computed by H,dand the
single style vector sas:
P(i) =σ(wh+wd+ws+
b),where h=/summationdisplayα(i)∗H
(4)
where w,w,w,bare learnable pa-
rameters. The final probability distribution of the
generated word ias a certain word wis:
P(i) =p(i)P(i)+
(1−p(i))/summationdisplayα(i)(5)5066
The final probability distribution is used to compute
the teacher forcing loss, and the final loss function
is:
Loss =L +λL
(6)
where λis a hyperparameter.
The two headline style vector fusion methods
take different ways to influence the final headline
generation. For a new article, there are many rea-
sonable headlines for the article from content to
syntax. The first one is similar to informing the de-
coder the desired headline style in advance, allow-
ing the decoder to have a more explicit generation
direction. Besides, headlines with different author
styles have different word preferences. The second
one can guide the choice of words in the pointer
network and whether to use pointer or generator.
4 Experiments
4.1 Dataset
We collect our H3G dataset from an online social
media platform Tencent QQBrowser. Some plat-
form accounts are shared by more than one authors
and publish a large number of articles every day. So
we select the accounts who published 3-60 articles
within two months in 2021. Finally, we get more
than 380K different articles of more than 23K dif-
ferent authors, and the statistics of the H3G dataset
are shown in Table 1. We randomly divide the H3G
dataset into training set, validation set and test set.
The validation set and test set contain 500 and 2000
samples respectively, and the rest of the articles are
used as the training set. For these three sets, we
search the historical headlines of the same author
from the headlines in the training set, which avoids
the answers leakage of the validation set and the
test set. In this paper, we do not consider the time
when the article was published, so the historical
headlines are all headlines within two months from
the same author, excluding the target headline.
4.2 Baselines
We select two competitive models as our basic base-
line models, namely general HG and merge H3G.
•General HG model uses transformer architec-
ture and BERT-base to initialize the encoderparameters as our CLH3G. Different from our
CLH3G model, the general HG model only
use the original article to generate the corre-
sponding headline. The general HG model is
used to verify the effectiveness of historical
headlines for headline generation.
•Compared with General HG model, the merge
H3G model concatenates historical headlines
and the article as the input of encoder, which
is a very simple method to utilize the histor-
ical headlines and can be used to verify the
effectiveness of our proposed CLH3G model.
Besides, we also implement two strong baseline
models, namely AddFuse HG, StackFuse HG from
(Liu et al., 2020).
•The AddFuse HG model concatenates all his-
torical headlines into a sentence as the input of
the encoder to get H .H and
Hare used to compute headline-filtered arti-
cleHthrough the multi-head self-attention
sub-layer. Finally, the target headline is gener-
ated by Hinstead of H.
•Based on the AddFuse HG model, the Stack-
Fuse HG model performs a multi-head atten-
tion on HandHone by one in each block
of the decoder, so each decoder stack is com-
posed of four sub-layers.
4.3 Implementation and Hyperparameters
We set the maximum article length and target head-
line length as 512 and 32 for all models. The
length of concatenated headlines in the AddFuse
H3G model and the StackFush H3G model is
256. The length of each historical headline in the
CLH3G model is 32. In order to be consistent
with the real online applications, the number of
historical headlines is random chosen from 1 to
min(K,#(articles of the author )−1), where
Kis a hyperparameter. The encoder and the de-
coder of all transformer-based models have the
same architecture hyperparameters as BERT-base.
The parameters of all models are trained by Adafac-
tor optimizer (Shazeer and Stern, 2018), which can
save the storage and converge faster. At the same
time, we set batch size and dropout of all models
to 96 and 0.1, respectively. We train all the models
50K steps and then test on the validation set every
500 steps. We finally report the results of the test
set in the best step of the validation set. During5067
inference, we also use beam search with length
penalty to generate more fluent headlines. We set
the beam size and length penalty of all models to 4
and 1.5, respectively.
4.4 Experimental Results on ROUGE and
BLEU
We use ROUGE (Lin, 2004) and BLEU (Papineni
et al., 2002) as metrics to automatically evaluate
the quality of generated headlines of all baseline
models and our CLH3G model. For all historical
headlines based HG models, we set the maximum
number of historical headlines Kto 10, and the λ
of our CLH3G model to 0.1. As shown in Table 2,
most of the H3G models are better than the general
HG model, which demonstrates the effectiveness
of historical headlines in headlines generation. The
merge H3G model has some similar results with
the general HG model, because we only truncate
the article to keep the input length of the merge
H3G model as 512, and the long historical head-
lines causes the loss of the article information. Be-
sides, the two strong baseline models AddFuse HG
model and StackFuse HG model achieve excellent
results compared with the general HG model and
the merge H3G model. There are two main rea-
sons for this: (1) these two models can obtain addi-
tional historical headlines information than the gen-
eral HG model; (2) compared with the merge H3G
model, the information of the original article will
not be lost when using historical headlines. Com-
pared with the AddFuse H3G model, the StackFuse
H3G model uses the original article representations
Hincrementally and perform better results. Fi-
nally, our CLH3G model achieves the best results
for all metrics, which demonstrates our CLH3G
can extract and utilize the information of historical
headlines effectively compared with other baseline
models. Besides, the complexity per layer of the
self-attention model is O(n·d), and our CLH3G
model represents all historical headlines one by
one, while the AddFuse H3G model and the Stack-
Fuse model represents all concatenated historicalheadlines at the same time, so our CLH3G model
is more efficient than AddFuse H3G model and
StackFuse model.
4.5 Experiment results on headline style
To study the style relationship between the gen-
erated headlines and the historical headlines, we
use BERT-base and Contrastive Learning to train
a classifier to distinguish headlines from different
authors. The setting of the classification model is
same as the contrastive learning based auxiliary
task in our CLH3G model. The samples in the
training set is a set of headlines of the same author,
and we randomly select two of them as the positive
samples to train the contrastive learning classifier.
The two headlines of the negative sample in the
validation set and the test set are randomly selected
from different authors. We train the contrastive
learning based classifier for 50K steps and obtain
the best model in the validation set according to ac-
curacy. The contrastive learning classifier will out-
put a score within [−1,1], and the higher the score
is, the greater the possibility that the two samples
belong to the same author. We make the generated
headline and all the historical headlines to build
the evaluation samples one by one and report the
accuracy and the average classification score. We
name the original author headline and the historical
headlines as Reference, which will get the highest
accuracy and average score in theory.
The classification accuracy and the average
scores are shown in Table 3. The Reference gets
the highest accuracy and average score compared
with other HG and H3G models. The general
HG model obtains the worst accuracy and aver-
age score, which is consistent with its performance
on ROUGE and BLEU, because it can only gen-
erate headlines aimlessly without the information
of historical headlines. We notice that the merge
H3G model achieves the best accuracy and aver-
age score besides Reference. This may be because
the merge H3G model exploits the whole historical
headlines and a small portion of the article to gen-
erate a new headline, and the missing information
of the article makes the model relies more on his-
torical headlines. Compared with the general HG
model, the AddFuse H3G model and the StackFuse
H3G model get better results, which is also consis-
tent with its performance on ROUGE and BLEU.
Our CLH3G model get approximate results com-
pared with the merge H3G model, and is better than5068
the AddFuse H3G model and the StackFuse H3G
model. The results of the classification accuracy
and average score can reflect the effectiveness of
using historical headlines. Our contrastive learning
module and two headline vector fusion methods
are both beneficial to learn the style of historical
headlines, resulting better accuracy and average
score. The best results on ROUGE and BLEU of
our CLH3G model prove that our CLH3G model
can utilize and fuse the article and the historical
headlines effectively at the same time.
4.6 Human Evaluation
Besides, we also apply Human Evaluation to verify
the generated headine style. We randomly sampled
50 news from the test set and asked three anno-
tators to rerank the five generated headline and
the reference headline, while the ranked first get 6
points, and the ranked last get 1 point. Besides, The
similar headlines will get the same ranked points,
resulting the relatively high scores for all models.
We use three criteria namely fluency, relevance and
attraction as (Jin et al., 2020).
The results is shown in the Table 4. Similarly
with the results on Rouge, BLEU and the CL based
classification, the general HG get the worst results,
and the reference get the best results. The histori-
cal headlines based models get significantly better
results than the general HG model on fluency and
relevance. The historical headlines can guide the
generation of target headline syntax, resulting bet-
ter fluency. Meanwhile, the better relevance is be-
cause these historical headlines based models have
less factual consistency errors than the general HG
model. Finally, our CLH3G model get the best
results on all three aspects except the Reference
headlines.
4.7 Experimental Results with different
values of hyperparameter λ
In order to study the influence of contrastive learn-
ing module, we train our CLH3G model with dif-
ferent contrastive learning coefficient λ. We report
the BLEU results of the experiments in Figure 3.
The different values of λhave a great impact on
the final results of our CLH3G model, and a clear
conclusion can be drawn from the results. When λ
is smaller than 0.1, the larger of λ, the better of the
performance. And when λis bigger than 0.1, the
smaller of λ, the better of the performance. The
best result is achieved when λis 0.1. We will ana-
lyze the reasons of this experiment results. Firstly,
when λis very small, contrastive learning module
has little positive impact on the whole model, so
that the results are getting better. Then, with the in-
creasing of λ, contrastive learning module has too
much impact on the whole model, which disturbs
the training of headline generation, so the results
are getting worse.
4.8 Incremental Experiments
To further demonstrate the effectiveness of con-
tractive learning module and the two headline vec-
tor fusion methods in our CLH3G model, we con-
duct incremental experiments and report the results
in Table 4. As shown in Table 6, the concat and5069
pointer headlines style vector fusion methods both
can improve the performance of Headline Gener-
ation, because they use additional historical head-
lines. In addition, the concat fusion method can
get better results compared with the pointer fusion
method, which proves that informing the decoder
the desired headline style in advance is more ef-
fective than guiding the choice of words in pointer
network. It may also be due to that there are few
overlapping words between different headlines of
the same author, and their headline patterns and
style are consistent instead. We also add contrastive
learning based auxiliary task to the concat fusion
method and the pointer fusion method, respectively.
The performance of the concat fusion method us-
ing CL based auxiliary task is slightly improved in
ROUGE-1 and ROUGE-2, while the performance
in ROUGE-L and BLEU is reduced, which shows
that CL has little effect on the concat fusion method.
The pointer fusion method with the CL based aux-
iliary task greatly improves the effectiveness of all
metrics, which proves that the pointer is more de-
pendent on the headline style. Furthermore, when
we use the two fusion methods at the same time,
the results is somewhere in between using a singlemethod, because the relatively worse headline vec-
tor will mislead the choice of words in the pointer.
For our CLH3G model, the better headline vec-
tor leads to the best results, which demonstrates
our contrastive learning module can extract better
headline style vector for H3G models.
4.9 Case Study
We display an example of generated headlines by
general HG model and CLH3G model in Table
5. Both the historical headlines and the generated
headline by CLH3G model are exclamatory sen-
tences. Besides, the generated headline by CLH3G
is more informative and attractive than the gener-
ated headline by general HG model.
5 Conclusion
In this paper, we discuss the effectiveness of His-
torical Headlines for Headline Generation, and aim
to generate headlines not only appropriate for the
given news articles, but consistent with the author’s
style. We build a large Historical Headlines based
Headline Generation dataset, and propose a novel
model CLH3G to integrate the historical headlines
effectively, which contains a contrastive learning
based auxiliary task and two headline style vec-
tor fusion methods. Experimental results show the
effectiveness of historical headlines for headline
generation and the exceptional performance of both
the CL based auxiliary task and the two headline
style vector fusion methods of our CLH3G model.5070Limitations
This paper introduces a new headline generation
task, which use historical headlines to generate ar-
ticle headlines, and also proposes a novel model
called CLH3G for this task. CLH3G uses two head-
line style vector fusion methods to make full use
of historical headlines. However, those two style
vector fusion methods is difficult to applied into
pretrained Sequence to Sequence model including
T5, Mass (Song et al., 2019; Raffel et al., 2019) di-
rectly, because those two methods will change the
whole architecture of pretrained model, resulting
slightly worse results compared with original pre-
trained model. As result, the integration of CLH3G
and pretrained Sequence to Sequence models re-
quires abundant H3G data to achieve comparable
results.
Acknowledgements
We would like to thank the anonymous reviewers
for their constructive comments.
References50715072