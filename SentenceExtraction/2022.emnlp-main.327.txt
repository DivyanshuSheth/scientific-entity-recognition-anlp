
Haochen LiChunyan MiaoCyril LeungYanxian Huang
Yuan HuangHongyu ZhangYanlin WangSchool of Computer Science and Engineering, Nanyang Technological University, SingaporeChina-Singapore International Joint Research Institute (CSIJRI), ChinaSchool of Software Engineering, Sun Yat-sen University, ChinaThe University of Newcastle, Australia
{haochen003,ascymiao,cleung}@ntu.edu.sg, huangyx353@mail2.sysu.edu.cn
{huangyuan5,wangylin36}@mail.sysu.edu.cn, hongyu.zhang@newcastle.edu.au
Abstract
Code search, which aims at retrieving the most
relevant code fragment for a given natural lan-
guage query, is a common activity in software
development practice. Recently, contrastive
learning is widely used in code search research,
where many data augmentation approaches
for source code (e.g., semantic-preserving pro-
gram transformation) are proposed to learn
better representations. However, these aug-
mentations are at the raw-data level, which
requires additional code analysis in the pre-
processing stage and additional training costs
in the training stage. In this paper, we ex-
plore augmentation methods that augment data
(both code and query) at representation level
which does not require additional data pro-
cessing and training, and based on this we
propose a general format of representation-
level augmentation that unifies existing meth-
ods. Then, we propose three new augmenta-
tion methods (linear extrapolation, binary inter-
polation, and Gaussian scaling) based on the
general format. Furthermore, we theoretically
analyze the advantages of the proposed aug-
mentation methods over traditional contrastive
learning methods on code search. We experi-
mentally evaluate the proposed representation-
level augmentation methods with state-of-the-
art code search models on a large-scale pub-
lic dataset consisting of six programming lan-
guages. The experimental results show that
our approach can consistently boost the perfor-
mance of the studied code search models. Our
source code is available at https://github.
com/Alex-HaochenLi/RACS .
1 Introduction
In software development, developers often search
and reuse commonly used functionalities to im-
prove their productivity (Nie et al., 2016; Shuai
et al., 2020). With the growing size of large-scale
codebases such as GitHub, retrieving semanticallyrelevant code fragments accurately becomes in-
creasingly important in this field (Allamanis et al.,
2018; Liu et al., 2021).
Traditional approaches (Nie et al., 2016; Yang
and Huang, 2017; Rosario, 2000; Hill et al., 2011;
Satter and Sakib, 2016; Lv et al., 2015; Van Nguyen
et al., 2017) leverage information retrieval tech-
niques to treat code snippets as natural language
text and match certain terms in code with queries,
hence suffering from the vocabulary mismatch
problem (McMillan et al., 2011; Robertson et al.,
1995). Deep siamese neural networks first embed
queries and code fragments into a joint embedding
space, then measure similarity by calculating dot
product or cosine distance (Lv et al., 2015; Cam-
bronero et al., 2019; Gu et al., 2021). Recently,
with the popularity of large scale pre-training tech-
niques, some big models for source code (Guo
et al., 2021; Feng et al., 2020; Guo et al., 2022;
Wang et al., 2021; Jain et al., 2021; Li et al., 2022)
with various pre-training tasks are proposed and
significantly outperform previous models.
Contrastive learning is widely adopted by the
above-mentioned models. It is suitable for code
search because the learning objective aims to push
apart negative query-code pairs and pull together
positive pairs at the same time. In contrastive learn-
ing, negative pairs are usually generated by In-
Batch Augmentation (Huang et al., 2021). For pos-
itive pairs, besides labeled ones, some researchers
proposed augmentation approaches to generate
more positive pairs (Bui et al., 2021; Jain et al.,
2021; Fang et al., 2020; Gao et al., 2021; He
et al., 2020). The main hypothesis behind these
approaches is that augmentations do not change the
original semantics. However, these approaches are
resource-consuming (Yin et al., 2021; Jeong et al.,
2022). Models have to embed the data again for
the augmented data.
To solve this problem, some researchers pro-
posed representation-level augmentation, which4924augments the representations of the original data.
For example, linear interpolation, a representation-
level augmentation method, is adopted by many
classification tasks in NLP (Guo et al., 2019; Sun
et al., 2020; Du et al., 2021). The augmented rep-
resentation captures the structure of the data man-
ifold and hence could force model to learn better
features, as argued by Verma et al. (2021). These
augmentation approaches are also considered to be
semantic-preserving.
The representation-level augmentation methods
are not investigated on the code search task before.
To the best of our knowledge, Jeong et al. (2022)
is the only work to bring representation-level aug-
mentation approaches to a retrieval task. Besides
linear interpolation, it also proposes another ap-
proach called stochastic perturbation for document
retrieval. Although these augmentation methods
bring improvements to model performance, they
are not yet fully investigated. The relationships
between the existing methods and how they affect
model performance remain to be explored.
In this work, we first unify linear interpolation
and stochastic perturbation into a general format
of representation-level augmentation. We further
propose three augmentation methods (linear extrap-
olation, binary interpolation, and Gaussian scal-
ing) based on the general format. Then we theo-
retically analyze the advantages of the proposed
augmentation methods based on the most com-
monly used InfoNCE loss (Van den Oord et al.,
2018). As optimizing InfoNCE loss equals to max-
imizing the mutual information between positive
pairs, applying representation-level augmentation
leads to tighter lower bounds of mutual informa-
tion. We evaluate representation-level augmenta-
tion on several Siamese networks across several
large-scale datasets. Experimental results show
the effectiveness of the representation-level aug-
mentation methods in boosting the performance
of these code search models. To verify the gener-
alization ability of our method to other tasks, we
also conduct experiments on the paragraph retrieval
task, and the results show that our method can also
improve the performance of several paragraph re-
trieval models.
In summary, our contributions of this work are
as follows:
•We unify previous representation-level aug-
mentation methods to propose a general for-
mat. Based on this general format, we proposethree novel augmentation methods.
•We conduct theoretical analysis to show that
representation-level augmentation has tighter
lower bounds of mutual information between
positive pairs.
•We apply representation-level augmentation
on several code search models and eval-
uate them on the public CodeSearchNet
dataset with six programming languages.
Improvement of MRR (Mean Reciprocal
Rank) demonstrates the effectiveness of the
representation-level augmentation methods.
The rest of the paper is organized as follows.
We introduce related work of code search and
data augmentation in Section 2. Section 3 in-
troduces the main part, including the general for-
mat of representation-level augmentation, new aug-
mentation methods, and their application on code
search. In Section 4, we analyze the theoretical
lower bounds of mutual information and study why
our approach works. In Section 5 and Section 6,
we conduct extensive experiments to show the ef-
fectiveness of our approach. Then we discuss the
generality of our approach in Section 7 and Sec-
tion 8 concludes this paper.
2 Related Work
2.1 Code search
As code search can significantly improve the pro-
ductivity of software developers by reusing func-
tionalities in large codebases, finding the semantic-
relevant code fragments precisely is one of the key
challenges in code search.
Traditional approaches leverage information re-
trieval techniques that try to match some keywords
between queries and codes (McMillan et al., 2011;
Robertson et al., 1995). These approaches suffer
from vocabulary mismatch problem where mod-
els fail to retrieve the relevant codes due to the
difference in semantics.
Later, deep neural models for code search are
proposed. They could be divided into two cate-
gories, early fusion and late fusion. Late fusion
approaches (Gu et al., 2018; Husain et al., 2019)
use a siamese network to embed queries and codes
into a shared vector space separately, then calcu-
late dot product or cosine distance to measure the
semantic similarity. Recently, following the idea of
late fusion, some transformer-based models with4925specifically designed pre-training tasks are pro-
posed (Feng et al., 2020; Guo et al., 2021, 2022).
They significantly outperform previous models by
improving the understanding of code semantics.
Instead of calculating representations of queries
and codes independently, early fusion approaches
model the correlations between queries and codes
during the embedding process (Li et al., 2020). Li
et al. (2020) argues that early fusion makes it easier
to capture implicit similarities. For applications of
an online code search system, late fusion approach
facilitates the use of neural models because the
code representations can be calculated and stored
in advance. During run time, only query represen-
tations need to be computed. Thus, in this work,
we focus on late fusion approaches.
2.2 Data augmentation
Data augmentation has long been considered cru-
cial for learning better representations in con-
trastive learning. The augmented data are consid-
ered to have the same semantics with the original
data. For the augmentation of queries, synonym
replacement, random insertion, random swap, ran-
dom deletion, back-translation, spans technique
and word perturbation can be potentially used to
generate individual augmentations (Wei and Zou,
2019; Giorgi et al., 2021; Fang et al., 2020). For
the augmentation of code fragments, Bui et al.
(2021) proposed six semantic-preserving transfor-
mations: Variable Renaming ,Permute Statement ,
Unused Statement ,Loop Exchange ,Switch to If
andBoolean Exchange . These query and code
augmentation approaches have one thing in com-
mon, that is, the transformation is applied to the
original input data. Another category is augment-
ing during the embedding process. Models can
generate different representations of the same data
by leveraging time-varying mechanisms. MoCo
(He et al., 2020) encodes data twice by the same
model with different parameters. SimCSE (Gao
et al., 2021) leverages the property of dropout lay-
ers by randomly deactivating different neurons for
the same input. Methods described in this para-
graph are resource-consuming because models em-
bed twice to get representations of original data
and augmented one.
For representation-level augmentation on NLP
tasks, linear interpolation is widely used on classifi-
cation tasks in previous work (Guo et al., 2019; Sun
et al., 2020; Du et al., 2021). They take the interpo-lation result as noised data and want models to clas-
sify the noised one into the original class. Verma
et al. (2021) theoretically analyzed how interpola-
tion noise benefits classification tasks and why it
is better than Gaussian noise. Jeong et al. (2022)
is the first to introduce linear interpolation and per-
turbation to the document retrieval task. However,
the effect and intrinsic relationship of these two
methods are not fully investigated.
3 Approach
In this section, we unify the linear interpolation
and stochastic perturbation into a general format.
Based on it, we propose three other augmentation
methods for the code retrieval task, linear extrap-
olation, binary interpolation and Gaussian scaling.
Then, we explain how to apply representation-level
augmentation with InfoNCE loss in code retrieval.
3.1 General format of representation-level
augmentation
For simplicity, we take code augmentation as an
example to elaborate the details. The calculation
process is similar when applying to query augmen-
tations. Given a data distribution D={x}
where xis a code snippet, Kis the size of the
dataset. We use an encoder function h:D→H
to map codes to representations H.
Linear interpolation Linear interpolation ran-
domly interpolate hwith another chosen sample
hfromH:
h=λh+ (1−λ)h (1)
where λis a coefficient sampled from a random
distribution. For example, λcan be sampled from
a uniform distribution λ∼U(α,1.0)with high
values of αto make sure that the augmented data
has similar semantics with the original code x.
Stochastic perturbation Stochastic perturbation
aims at randomly deactivating some features of
representation vectors. In order to do so, masks
are sampled from a Bernoulli distribution B(e, p),
where eis the embedding dimension. pis a low
probability value since we only deactivate a small
proportion of features. For implementation, we
could use Dropout layers.
General format of representation-level augmen-
tation We revisit the above two augmentation
approaches and unify them into a general format,
which could be described as:4926h=α⊙h+β⊙h(2)
where h, h∈H,αandβare coefficient vectors.
For linear interpolation, α=λ,β= 1−λ,h, h∈
H, and h̸=h. For stochastic perturbation, α∈
{0,}where elements of αare sampled from a
Bernoulli distribution B(p),β=−α,h∈H,
andh= 0.
3.2 New augmentation methods
Based on the general format, we provide three new
augmentation methods for the code retrieval task.
Binary interpolation Binary interpolation ran-
domly swaps some features with another chosen
sample. The difference between binary interpola-
tion and stochastic perturbation is that the former
swaps with other samples while the latter swaps
with zero vector. Specifically, for binary interpola-
tion,α∈ {0,1}where elements of αare sampled
from a Bernoulli distribution B(p),β= 1−α,
h, h∈H, and h̸=h.
Linear extrapolation Wang and Isola (2020)
concludes that optimizing contrastive loss makes
feature vectors roughly uniformly distributed on
the hypersphere. As linear interpolation generates
augmented data inside the hypersphere, linear ex-
trapolation oppositely generates outside ones. λis
sampled from a uniform distribution λ∼U(1.0, α)
with small values of α. Other settings are the same
as linear interpolation.
Gaussian scaling Gaussian scaling generates
scaling coefficients for each feature in the repre-
sentation, which can be considered as a type of
perturbation noise. Compared with directly adding
Gaussian noise, the proposed scaling noise cap-
tures the structure of the data manifold, which may
force networks to learn better representations. If
we describe Gaussian scaling in general format,
thenα= 1,β∼N(0, σ)with small values of σ,
h=h∈H.
3.3 Contrastive learning with
representation-level augmentation for
code search
Contrastive learning seeks to satisfy that similar-
ities between positive pairs are greater than that
between negative pairs, which is suitable for code
search. In previous works, in order to optimize
the objective, several loss functions are proposed,including triplet loss (Mikolov et al., 2013), max-
margin loss (Hadsell et al., 2006), and logistic loss
(Weinberger and Saul, 2009). In this work, we
consider InfoNCE loss (Van den Oord et al., 2018)
because of its better performance hence dominant
adaption in current contrastive learning models.
For the effect of representation-level augmentation
on other loss functions, we empirically analyze it
in Appendix B.
We start from the vanilla InfoNCE loss. Suppose
we have a batch of Bsamples consisting of queries
and codes. We encode queries and codes to get
query representations Q={q}and code repre-
sentations C={c}using the encoder function
h.(q, c)are positive pairs when i=jand nega-
tive pairs otherwise. Therefore, for each query, we
could generate 1 positive pair and B−1negative
pairs. The InfoNCE loss tries to maximize the sim-
ilarity between one positive pair and minimize the
similarity between other negative pairs. Here we
use dot product as the measurement of similarity
and in Section 6.4 we will discuss the effect of us-
ing dot product compared to cosine distance. The
loss can be described as:
L=−E/bracketleftigg
logexp(q·c)
exp(q·c) +/summationtextexp(q·c)/bracketrightigg
(3)
Then, we conduct representation-level augmen-
tation. We randomly choose one out of the five
augmentation methods and augment the original
queries Qand original codes CforNtimes. In
each augmentation, the augmentation approach is
fixed, but the coefficients αandβare randomized
hence different. After augmentation, we get aug-
mented query and code sets, Q={q}
andC={c}. We follow the hy-
pothesis of other representation-level augmenta-
tion methods that the augmented representation
still preserves or is similar to the original seman-
tics. Therefore, for a certain query q, we consider
qandq,∀n∈[1, N]share the same semantic
meaning. And this similarly applies to candc,
∀n∈[1, N]. Since (q, c)is labeled as a positive
pair when i=j,∀n∈[1, N] (q, c)and(q, c)
are also naturally labeled as positive pairs. Simi-
larly, we get (q, c)as negative pairs. Thus,
compared with vanilla InfoNCE loss, we now have
(N+1)Bpositive pairs in total and for each query
q∈Q∪Qwe can generate (B−1)(N+ 1) neg-
ative pairs.49274 Theoretical Analysis
In this section, we mathematically analyze the ef-
fect of InfoNCE loss with or without representation-
level augmentation and prove that optimizing In-
foNCE loss with representation-level augmenta-
tion leads to mutual information with tighter lower
bounds between positive pairs. Here we take lin-
ear interpolation as an example to demonstrate the
benefits. Other forms of representation-level aug-
mentation are left to future work. The mutual in-
formation between a query qand a code fragment
cis:
I(q, c) =E/bracketleftbigg
logp(c|q)
p(c)/bracketrightbigg
(4)
Theorem 1 Optimizing InfoNCE loss Lim-
proves lower bounds of mutual information I(q, c)
for a positive pair:
I(q, c)≥log(B)− L (5)
where q∈Q,c∈C, and Bis the size of sets.
Proof of Theorem 1 is presented in Appendix
A.1. This is proved in the original paper of In-
foNCE loss (Van den Oord et al., 2018). Here, to
better extend the proof of Theorem 2, we prove it
in another way.
Theorem 2 Optimizing InfoNCE loss Lwith
representation-level augmentation improves a
tighter lower bounds of mutual information I(q, c)
for a positive pair:
I(q, c)≥1
α(log(NB)− L
−αβ·I(q, c)−αβ·I(q, c)
−β·I(q, c))(6)
where q, q∈Q,c, c∈C,(q, c),(q, c)
and(q, c)are all negative pairs, αandβare
coefficients in Equation 2, Bis the size of sets, and
Nis the augmentation time.
Proof of Theorem 2 is presented in Appendix
A.2. Since we interpolate qwith other samples q
in the batch ( cwithc), the mutual information
between (q, c),(q, c)and(q, c)are also in-
corporated into the optimizing process. As defined
in Eq.1, βis a small value that close to 0. Ac-
cording to Eq.4, for negative pairs p(c|q)can be
expressed asdue to the independence of
sampling qandc. Considering this, we can see thatLanguage Training Validation Test Codebase
Ruby 24,927 1,400 1,261 4,360
JavaScript 58,025 3,885 3,291 13,981
Go 167,288 7,325 8,122 28,120
Python 251,820 13,914 14,918 43,827
Java 164,923 5,183 10,955 40,347
PHP 241,241 12,982 14,014 52,660
the optimal mutual information between negative
pairs is also 0. Note that we interpolate qandc
with different samples to make sure that (q, c)
is a negative pair. Thus, the last three terms in Eq.6
can be ignored. Comparing(log(NB)− L)
withlog(B)− L, we can see that representation-
level augmentation improves the lower bounds of
mutual information.
5 Experimental Setup
In this section, we describe datasets, baselines, and
implementation details.
5.1 Datasets
To evaluate the effectiveness of representation-
level augmentation, we use a large-scale bench-
mark dataset CodeSearchNet (CSN) (Husain et al.,
2019) that is widely used in previous studies (Guo
et al., 2021; Feng et al., 2020; Guo et al., 2022).
It contains six programming languages including
Ruby, JavaScript, Go, Python, Java, and PHP. The
statistics of the dataset are shown in Table 1. For
the training set, it contains positive-only query-
code pairs. For validation and test sets, they only
have queries and the model retrieves the correct
code fragments from a fixed codebase. Here we
follow (Guo et al., 2021) to filter out low-quality
examples (such as code that cannot be successfully
parsed into Abstract Syntax Trees).
We measure the performance using Mean Re-
ciprocal Rank (MRR) which is widely adopted in
previous studies. MRR is the average of reciprocal
ranks of a true code fragment for a given query Q.
It is calculated as:
MRR =1
|Q|/summationdisplay1
Rank(7)
where Rankis the rank of the correct code frag-
ment that is related to the i-th query.49285.2 Baselines
Since representation-level augmentation is orthog-
onal to siamese networks, we apply it to several
models:
•RoBERTa (code) is pre-trained with mask
language modeling (MLM) task on code cor-
pus (Husain et al., 2019).
•CodeBERT is a bi-modal pre-trained model
pre-trained on two tasks: MLM and replaced
token detection (Feng et al., 2020). Note
that in this work we refer CodeBERT to the
siamese network architecture described in the
appendix of the original paper.
•GraphCodeBERT takes the structure in-
formation of codes into account. (Guo
et al., 2021) develops two structure-based pre-
training tasks: data flow edge prediction and
node alignment.
•UniXCoder leverages cross-model contents
like AST and comments to enhance code rep-
resentation (Guo et al., 2022).
5.3 Implementation details
For all the settings of these models except the
training epoch, we follow the original paper. For
representation-level augmentation, since linear ex-
trapolation and linear interpolation is similar, we
implement it as one approach. During training,
we randomly choose one augmentation approach
out of four with equal probability for a batch and
augment data 5 times. We re-sample data and co-
efficients to augment the original data in each aug-
mentation. Specifically, for linear interpolation and
extrapolation, we sample αfrom a uniform distri-
bution U∼(0.9,1.1). For perturbation, we set
the probability pof the Dropout layer as 0.1. For
binary interpolation, we sample αfrom a Bernoulli
distribution B(p= 0.25). For Gaussian scaling,
we sample βfrom a normal distribution N(0,0.1).
We set the training epoch as 30. All experiments
are conducted on a GeForce RTX A6000 GPU.
6 Results
In this section, we first show the overall
performance on code search when applying
representation-level augmentation, and then indi-
vidually analyze each augmentation method. Then,
we demonstrate the relationship between loss andMRR and the effect of augmentation on vector dis-
tribution. Finally, we take an ablation study to
analyze the impact of augmentation times.
6.1 Overall results
The overall performance evaluation results are
shown in Table 2. In each iteration, we randomly
choose one augmentation method. We can see that
representation-level augmentation is a universal
approach that can consistently improve the code
search performance. Optimizing a tighter lower
bound of mutual information brings about 2% gain
of MRR on average. The robust improvements
have no relationships with certain models or cer-
tain programming languages.
6.2 Effectiveness of individual augmentation
approach
To evaluate the effectiveness of the five augmenta-
tion approaches, we apply them alone and test them
on the CSN-Python dataset, as shown in Table 3.
Note that we follow the same experimental settings
described in Section 5. As the results show, ev-
ery augmentation approach can improve baselines
and the improvements brought by these augmen-
tation approaches are stable. The combination of
these approaches does not boost the performance
compared with individually applying one of these
augmentations. Since all these approaches can be
derived from the general format, they have no dis-
tinct difference and hence share the similar effect
on improving the mutual information between pos-
itive pairs.
6.3 Relationship between Loss and MRR
In theoretical analysis, we get the conclusion that
the relationship between the mutual information
of positive pairs and loss is I(q, c)≥log(B)−
Lwhile for representation-level augmentation4929
I(q, c)≥(log(NB)− L). Thus, we argue
that the effect of representation-level augmentation
is improving the lower bounds of mutual informa-
tion. However, this only comes true when loss can
decrease to similar values under such two condi-
tions. To prove that, after each epoch, we save
the model parameters, record the loss of the train-
ing set, test models on the CSN-Python test set,
and plot the relationship between loss and MRRwith or without representation-level augmentation,
as shown in Figure 1. We can see that when the
loss is the same, representation-level augmentation
always leads to better performance (GraphCode-
BERT). Even when loss cannot decrease to the
same value without augmentation, it outperforms
the vanilla contrastive learning (CodeBERT). We
believe that other than improving the lower bounds,
capturing the explicit relations between augmented
data and the original one can also lead to higher
mutual information between positive pairs.
6.4 Impact on vector distribution
In code search, we measure the similarity by cal-
culating the dot product between query represen-
tations and code representations. Here we analyze
the influence of representation-level augmentation
by visualizing the code vector distribution. We add
a linear layer to embed the representations to a two-
dimensional vector, as shown in Figure 2. Besides,
we plot the two-norms of vectors with Gaussian
kernel density estimation.
As Wang and Isola (2020) concluded, the op-
timization of InfoNCE loss makes vectors evenly
distributed, which corresponds to the left image
of Figure 2. Vectors roughly have the same two-
norm. After applying augmentation, vectors still
follow a circular pattern but their two-norms are4930
changed. We argue that the InfoNCE loss degrades
dot product to cosine distance since all vectors have
similar norms while representation-level augmen-
tation leverages the norms of vectors to distinguish
some hard examples. We can see that in the middle
image of Figure 2, vector norms are much more
uniformly distributed than that without augmenta-
tion.
However, this impact also reveals a limitation of
representation-level augmentation. Some previous
studies argue the necessity of using vector normal-
ization, otherwise, the Softmax distribution will be
made arbitrarily sharp. For example, the last step
of UniXCoder is normalization. We evaluate UniX-
Coder on CSN-Python, as shown in Table 4. When
we apply augmentations to UniXCoder with nor-
malization, the performance is worse. However, if
we remove the normalization step, augmentations
can boost performance just like for other models.
6.5 Impact of the number of augmentation
times
To analyze the impact of augmentation times N,
we conduct experiments on CodeBERT with N=
5,15,25, respectively. We take 10 epochs as an
interval, save the best model in each interval and
test them on the test set, as shown in Figure 3.
We can see that augmenting more times leads to a
relatively better performance that is even greater
than the result reported in Table 2. However, better
results require more training time. According to
I(q, c)≥(log(NB)− L), bigger Nis better,
but time cost of minimizing Lalso increases sig-
nificantly. As we could see in the figure, a 5-times
augmentation takes 30 epochs to converge while a
15-times augmentation takes 50 epochs. We train
CodeBERT with N= 25 for 60 epochs and MRR
is still increasing. Therefore, we argue that for the
application of representation-level augmentation,
we should find a balance between performance and
time cost.
7 Discussion
According to the proof of Theorem 2, the advan-
tage of applying representation-level augmentation
should be task-agnostic. To show its generalization
ability, we evaluate our proposed approaches on
two passage retrieval benchmark datasets, NFCor-
pus (Boteva et al., 2016) and FiQA-2018. The
difference between code search and passage re-
trieval is that the retrieved items are changed from
code snippets written in programming language to
passages written in English. We take DistilBERT
(Sanh et al., 2019) and RoBERTa (Liu et al., 2019)
for experiments. We implement our approach based
on an open-sourced framework BEIR (Thakur et al.,
2021). The two models are fine-tuned on two
datasets for 20 epochs, respectively. The settings
of augmentation are the same as those in the code
search task. For other hyper-parameters, we follow
the settings that are provided by the framework.
Results are shown in Table 5, which confirms that
representation-level augmentation also improves
the performance of passage retrieval models.
8 Conclusion
In this work, we unify existing approaches to pro-
pose a general format of representation-level aug-
mentation in code search. Based on the general
format, we propose three other augmentation meth-
ods. We further theoretically analyze the effect of
representation-level augmentation by proving that
it helps optimize a tighter lower bound of mutual in-
formation between positive pairs. We evaluate our4931approach on several models and datasets and the re-
sults demonstrate the effectiveness of the proposed
approach.
Limitations
As discussed in Section 6, representation-level aug-
mentation mainly has two limitations. First, it can-
not boost the performance for models with vector
normalization. We find that representation-level
augmentation improves performance by leveraging
the norms of vectors. With normalization, the norm
of vectors is fixed and hence augmentation cannot
bring performance gains. Second, as discussed
in Section 6.5, although augmenting more times
can lead to better performance, the time spent on
training also increases. Balancing performance and
time-cost will be our future work.
9 Acknowledgement
We thank the anonymous reviewers for their help-
ful comments and suggestions. This research is
supported in part by the National Research Founda-
tion, Prime Minister’s Office, Singapore under its
NRF Investigatorship Programme (NRFI Award
No. NRF-NRFI05-2019-0002). Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not reflect the views of National Research Foun-
dation, Singapore. This research is also supported
in part by the China-Singapore International Joint
Research Institute (CSIJRI), Guangzhou, China
(Award No. 206-A021002) and the Joint NTU-
WeBank Research Centre on Fintech (Award No.
NWJ-2020-007), Nanyang Technological Univer-
sity, Singapore.
References49324933A Proof
A.1 Proof of Theorem 1
According to the original paper of InfoNCE loss
(Van den Oord et al., 2018), the optimal value for
exp(q·c)is given by, by substituting it, we
can get:
L≥E/bracketleftig
log/bracketleftbigg
B·E[p(c|q)
p(c))]/bracketrightbigg/bracketrightig
−E/bracketleftig
logp(c|q)
p(c)/bracketrightig (9)
Since qandcare negative pairs and sam-
pled independently, E[] =E[] =
E[] = 1 . According to the definition of
mutual information described in Eq.4, the mutual
information I(q, c)is the second term. Thus, we
get:
L≥logB−I(q, c) (10)
Therefore, I(q, c)≥log(B)− L.
A.2 Proof of Theorem 2
By applying augmentation, the expectation of loss
can be divided into four parts that correspond to
four types of pairs: original query and original
code, augmented query and original code, original
query and augmented code, and augmented query
and augmented code. It can be expressed as:4934
where q∈Q,c∈C,q∈Q,c∈C,
andc∈C∪C. Next, we will analyze the
four terms individually. For the first term, original
query and original code, it is the same as proved
in Appendix A.1. For the second term, original
query and augmented code, the derivation can be
formulated as:
−E/bracketleftigg
logexp(q·c)
exp(q·c) +/summationtextexp(q·c)/bracketrightigg
=E/bracketleftigg
log/parenleftigg
1 +/summationtextexp(q·c)
exp(q·c)/parenrightigg/bracketrightigg
≥E/bracketleftigg
log/parenleftigg/summationtextexp(q·c)
exp(q·(α·c+β·c))/parenrightigg/bracketrightigg
=E/bracketleftigg
log/parenleftigg /summationtextexp(q·c)
exp(q·c·α)·exp(q·c·β))/parenrightigg/bracketrightigg
=E/bracketleftig
log/summationdisplayexp(q·c)−α·log exp( q·c)
−β·log exp( q·c)/bracketrightig
(12)
Similar to Eq.9, we substitute exponential func-
tion with probability, and we could get that the
second term is greater than logNB−αI(q, c)−
βI(q, c). The only difference between the second
term and the third term is that in the derivation of
second term we decompose cintoα·c+β·c
while for the third one we decompose qinto
α·q+β·q. Therefore, the third term is greater
thanlogNB−αI(q, c)−βI(q, c).
For the fourth term, it can be described as:
Then we remove the expectation of Eq.A.2 by
multiplying the corresponding probabilities of four
terms, we get:
Therefore, we get:
I(q, c)≥1
α/parenleftig
log(NB)− L
−αβ·I(q, c)−αβ·I(q, c)
−β·I(q, c)/parenrightig(15)
B Representation-level augmentation on
other loss functions
Here, we investigate whether representation-level
augmentation also benefits other loss functions. We
apply representation-level augmentation on triplet
loss (Mikolov et al., 2013) and logistic loss (Wein-
berger and Saul, 2009), because these two loss func-
tions are also used prior to InfoNCE loss. Triplet4935loss learns to maximize the distances between neg-
ative pairs while minimizing the distances between
positive pairs, which can be written as:
L =−1
N/summationdisplaymax(0,
||q−c||− ||q−c||+ϵ)(16)
And logistic loss is also called NCE loss, which
can be described as:
L =−1
N/summationdisplay/bracketleftig
logσ(q·c)
−1
N−1/summationdisplaylogσ(q·c)/bracketrightig(17)
where definitions of variables follow Equation 3,
σrepresents sigmoid function, and we set ϵ= 5in
our experiments.
We finetune CodeBERT and GraphCodeBERT
with the two loss functions for 20 epochs on Ruby
and Javascript dataset. Results of triplet loss and
logistic loss are shown in Table 6 and Table 7, re-
spectively.
As we can see, representation-level augmenta-
tion can also improve performance when other loss
functions are used. We believe this is because the
augmentation makes the model more robust.4936