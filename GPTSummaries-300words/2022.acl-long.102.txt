Summary:

This research paper explores reference-free evaluation metrics for natural language generation (NLG) systems, specifically focusing on text summarization and dialog generation. The authors find that existing metrics may rely on spurious correlations with measures such as word overlap, length, and perplexity. They demonstrate that these errors can be mitigated by explicitly designing evaluation metrics to avoid spurious features. The paper analyzes recently proposed metrics for text summarization and dialog generation and compares their performance against spurious correlates. The authors also propose an adversarial training strategy to develop a more robust metric. The evaluation results on several datasets show that the adversarially trained metric performs well in ranking systems within the abstractive and faithful group, indicating that it is possible to learn effective metrics that are not overly reliant on spurious correlations. The paper emphasizes the importance of assessing evaluation metrics across different distributions of systems and highlights the need for comprehensive human evaluation datasets. The research contributes to the understanding of reference-free evaluation metrics and their potential limitations.