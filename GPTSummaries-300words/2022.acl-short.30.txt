Summary: 
This research paper compares three transfer learning methods in natural language processing (NLP) - STILTs, pairwise MTL, and MTL - using the GLUE dataset suite. The authors find that pairwise MTL is better than STILTs when the target task has fewer instances, and vice versa. MTL is found to be worse than the pairwise methods in almost every case. The authors propose a simple size heuristic that accurately predicts which method to use in over 92% of cases. They also conduct experiments varying dataset size to validate this heuristic. The results support the size heuristic, showing that MTL becomes more effective than STILTs as the supporting task dataset size increases. The study provides insights for NLP researchers in choosing transfer learning methods for NLP tasks. The paper includes additional discussions on related work, theoretical explanations, and comparisons with previous studies. The experiments were conducted using the GLUE dataset suite and the DistilRoBERTa model.