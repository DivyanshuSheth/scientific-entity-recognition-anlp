Research Paper Summary:

This paper presents a method called Speech and Text Joint Pre-Training (STPT) for speech translation and recognition. The method incorporates four subtasks: (1) (Self-)supervised Text to Text (T2T) subtask, (2) Self-supervised Speech Learning (SSL) subtask, (3) Supervised Speech to Phoneme Classification (S2P) subtask, and (4) Supervised AED based Speech to Text (S2T) subtask. Two different model configurations, fully shared encoder (FSE) and partially shared encoder (PSE), are proposed to alleviate subtask interference. Experimental results show that the STPT method effectively fuses speech and text information and achieves significant improvements in speech translation and recognition tasks. The proposed method achieves between 1.7 and 2.3 BLEU improvement on the MST-C speech translation dataset and comparable Word Error Rates (WER) to the wav2vec 2.0 model on the L speech recognition task.

Task: Speech translation and recognition.
Dataset: Libri-light (unlabelled speech data), L dataset (labelled speech data), MST-C dataset (translation dataset).
Methods: STPT (Speech and Text Joint Pre-Training)
Hyperparameters: The paper does not explicitly mention specific hyperparameters that cannot be inferred while fitting models to the training data.
Evaluation Metrics: BLEU scores for translation task, WER for speech recognition task.
Evaluation Results: STPT achieves between 1.7 and 2.3 BLEU improvement on the MST-C speech translation dataset and comparable WERs to the wav2vec 2.0 model on the L speech recognition task.