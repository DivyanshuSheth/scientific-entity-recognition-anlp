Summary:

The paper proposes a training technique called Learning GoodTeacher Matters (LGTM) to enhance knowledge distillation in teacher-student models. It introduces the concept of distillation influence, which quantifies the impact of distillation from each training sample on the student's generalization ability. By prioritizing samples that enhance the student's generalization ability, LGTM outperforms 10 common knowledge distillation baselines on 6 text classification tasks in the GLUE benchmark. The paper compares LGTM with meta distillation and demonstrates the benefit of incorporating distillation influence. It also presents the evaluation results of LGTM on the GLUE benchmark and analyzes the trend of distillation influence during training. The paper includes ablation studies to evaluate the effectiveness of finite difference approximation and different distillation objectives. Finally, it discusses the impact of the teacher's initial state and the order of updating the teacher and student models.