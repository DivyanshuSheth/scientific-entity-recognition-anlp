Summary:

This research paper investigates instruction tuning (IT) for training language models to learn from human instructions for zero-shot cross-task generalization. The authors propose Unlabeled Data Augmented Instruction Tuning (UDIT) to improve IT with unlabeled data. They construct pseudo-labeled data from unlabeled plain texts and conduct extensive experiments to show UDIT's effectiveness in various scenarios of tasks and datasets. Results show that using unlabeled data enhances instruction learning and UDIT outperforms other methods, improving zero-shot cross-task generalization. The paper also analyzes the key factors of UDIT and provides insights into using unlabeled data to improve instruction learning. The findings highlight the importance of domain diversity, instruction matching, and training data amount for effective instruction learning.