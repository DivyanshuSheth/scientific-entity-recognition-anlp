Summary:

This research paper explores the impact of pretraining on different numbers of languages on unseen target languages in natural language processing (NLP) tasks. The authors conduct experiments using pretrained multilingual language models and evaluate their performance on part-of-speech tagging, named entity recognition, and natural language inference tasks. They investigate the influence of the number of pretraining languages on zero-shot performance for unseen target languages and analyze the effect of model adaptation to target languages. The experiments show that choosing a diverse set of pretraining languages is crucial for effective transfer. Without model adaptation, increasing the number of pretraining languages improves performance on unrelated unseen target languages up to a certain point, after which performance plateaus. However, with model adaptation, pretraining on a larger number of languages often leads to further improvement. The research concludes that the optimal number of pretraining languages depends on the specific task and language family. The findings suggest that multilingual models can effectively transfer knowledge to unseen languages, especially with model adaptation. The results have implications for practical applications of cross-lingual transfer in NLP.