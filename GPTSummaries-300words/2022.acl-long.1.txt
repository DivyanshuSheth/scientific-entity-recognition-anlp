Summary:

This paper proposes a method called AdapLeR for reducing the computational cost of BERT models during inference. The method dynamically eliminates less contributing tokens through layers, resulting in shorter lengths and lower computational cost. To determine the importance of each token representation, the authors train a Contribution Predictor for each layer using a gradient-based saliency method. Experimental results on multiple classification tasks show speedups up to 22x during inference time without sacrificing performance. The authors also validate the quality of the selected tokens using human annotations in the ERASER benchmark. AdapLeR outperforms other widely used strategies for selecting important tokens and has a significantly lower false positive rate in generating rationales. The paper provides details of the methodology, hyperparameters, evaluation metrics, and results on various datasets. The proposed method has the potential to improve the efficiency of pre-trained language models in resource-limited settings.